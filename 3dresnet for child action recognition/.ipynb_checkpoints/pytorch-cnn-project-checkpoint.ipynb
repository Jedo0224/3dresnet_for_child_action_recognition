{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4304,
     "status": "ok",
     "timestamp": 1677478411884,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "6ao9txfslf7Q",
    "outputId": "88b14d39-46de-4828-f3a7-847847b70a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.9.0+cu111\n"
     ]
    }
   ],
   "source": [
    "## 첫번째 코드블록\n",
    "import torch \n",
    "print('pytorch version: {}'.format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1677478413010,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "cM8GGGuwlf7b",
    "outputId": "40fb2a47-a511-4d19-addf-2ce72a6b21ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.9.0+cu111\n",
      "GPU 사용 가능 여부: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image ,ImageSequence\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import check_util.checker as checker \n",
    "%matplotlib inline\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # GPU 사용 가능 여부에 따라 device 정보 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1677478414715,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "sIN5Cskylf7d"
   },
   "outputs": [],
   "source": [
    "data_dir = './data'  # 압축 해제된 데이터셋의 디렉토리 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1677478416810,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "btYA7dDXlf7i"
   },
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1677478418202,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "EFiLOwp6tXpB"
   },
   "outputs": [],
   "source": [
    "# basename=os.path.basename(\"/content/drive/MyDrive/02_cnn_pt/data/my_cat_dog/test/dog/dog.1503.jpg\")\n",
    "\n",
    "# print(basename.startswith(\"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1677478418699,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "FImsKjOalf7k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1677478419926,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "OIbAJFRjeEfY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1677478419926,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "tI6QadixpTTW"
   },
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    with Image.open(path) as img:\n",
    "        frames = []\n",
    "        for frame in ImageSequence.Iterator(img):\n",
    "            frames.append(frame.convert('RGB'))\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1677478628906,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "t1XLy4kzm5LK"
   },
   "outputs": [],
   "source": [
    "class CatDogDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode, transform=None):\n",
    "        self.all_data = sorted(glob.glob(os.path.join(data_dir, mode, '*','*')))\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.all_data[index]\n",
    "        gif = pil_loader(data_path)\n",
    "        \n",
    "        \n",
    "        frames = []\n",
    "#         frame_count=0\n",
    "#         for frame in gif:\n",
    "#             frame_count += 1\n",
    "\n",
    "#         print(\"{} : The number of frames in the image is {}\".format(data_path,frame_count))\n",
    "      \n",
    "     \n",
    "        for frame in (gif):\n",
    "            \n",
    "            frames.append(np.array(self.transform(frame)))\n",
    "    \n",
    "        # Stack the frames to create a 4D array\n",
    "        img = np.stack(frames, axis=0)\n",
    "        img = np.stack(img, axis=0)\n",
    "       \n",
    "        \n",
    "       # Normalize to the [0, 1] range\n",
    "        img = torch.FloatTensor(img)\n",
    "\n",
    "        # Step 2: Get the label for the image\n",
    "        basename = os.path.basename(data_path)  \n",
    "        \n",
    "        if basename.startswith(\"animation.0.\"):\n",
    "            label = 0 #\"서있기\"\n",
    "        elif basename.startswith(\"animation.1.\"):\n",
    "            label = 1 # \"걷기\"\n",
    "        elif basename.startswith(\"animation.2.\"):\n",
    "            label = 2   #\"구부리기\"\n",
    "        elif basename.startswith(\"animation.3.\"):\n",
    "            label = 3 #\"비틀기\"\n",
    "#         elif basename.startswith(\"animation.4.\"):\n",
    "#             label = 4#\"넘기\"\n",
    "#         elif basename.startswith(\"animation.5.\"):\n",
    "#             label = 5 #\"물건꺼내기\"\n",
    "        elif basename.startswith(\"animation.6.\"):\n",
    "            label = 4 #\"밀고당기기\"\n",
    "#         elif basename.startswith(\"animation.7.\"):\n",
    "#             label = 5 #\"달리기\"\n",
    "#         elif basename.startswith(\"animation.8.\"):\n",
    "#              label =8 #\"제자리점프\"\n",
    "#         elif basename.startswith(\"animation.9.\"):\n",
    "#             label = 9 #\"발차기\"\n",
    "        elif basename.startswith(\"animation.10.\"):\n",
    "            label = 5# \"기어다니기\"\n",
    "        elif basename.startswith(\"animation.11.\"):\n",
    "            label = 6# \"엎드려있기\"\n",
    "        elif basename.startswith(\"animation.12.\"):\n",
    "             label = 7 #\"앉기\"\n",
    "        \n",
    " \n",
    "       \n",
    "        \n",
    "  \n",
    "        \n",
    "        return img.reshape(3,30,60,60), label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1677478540095,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "CjpnJKqmE4_n"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1677478540096,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "N7n8AF-CTc_Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1677478540575,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "e-KZCbzNjRVl"
   },
   "outputs": [],
   "source": [
    "\n",
    "# class CatDogDataset(Dataset):\n",
    "#     def __init__(self, data_dir, mode, transform=None):\n",
    "#         self.all_data = sorted(glob.glob(os.path.join(data_dir, mode, '*','*')))\n",
    "#         self.transform = transform\n",
    "#         self.mode = mode\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         # Step 1: 반환할 이미지 경로 정의 및 이미지 로드\n",
    "#         ## 코드 시작 ##\n",
    "#         data_path = self.all_data[index]   # 위의 설명 Step 1의 1. 을 참고하여 None을 채우세요.\n",
    "#         img = Image.open(data_path)         # 위의 설명 Step 1의 2. 를 참고하여 None을 채우세요.\n",
    "#         img = self.transform(img)         # 위의 설명 Step 1의 3. 을 참고하여 None을 채우세요.\n",
    "        \n",
    "\n",
    "#         ## 코드 종료 ##\n",
    "        \n",
    "#         # Step 2: 이미지에 대한 label 정의\n",
    "#         ## 코드 시작 ##\n",
    "#         basename=os.path.basename(data_path)  \n",
    "#         if(basename.startswith(\"a3\"))  : \n",
    "#           label =0\n",
    "#         elif(basename.startswith(\"a4\")):\n",
    "#           label =1\n",
    "#         else:\n",
    "#           label =2           # 위의 설명 Step 2 를 참고하여 None을 채우세요.\n",
    "#         ## 코드 종료 ##\n",
    "#         return img, label\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         ## 코드 시작 ##\n",
    "#         length = len(self.all_data)\n",
    "#         ## 코드 종료 ##\n",
    "#         return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1677478540931,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "sr3WHtDgZnME"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1677478631536,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "GBJlIr2dlf7l"
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "\n",
    "        \n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "       \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "train_data = CatDogDataset(data_dir='/home/mhealth02/바탕화면/workspace/points/dataset_size_60_frame_30_', mode='train', transform=data_transforms['train'])\n",
    "val_data = CatDogDataset(data_dir='/home/mhealth02/바탕화면/workspace/points/dataset_size_60_frame_30_', mode='val', transform=data_transforms['val'])\n",
    "test_data = CatDogDataset(data_dir='/home/mhealth02/바탕화면/workspace/points/dataset_size_60_frame_30_', mode='test', transform=data_transforms['val'])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1677478518182,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "IKdFe1gTT8LA"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1677478518182,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "qmRLEqET3rsW"
   },
   "outputs": [],
   "source": [
    "# data_transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "#         transforms.RandomRotation(5),\n",
    "#         transforms.Resize([120, 120]),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomResizedCrop(120, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "#     'val': transforms.Compose([\n",
    "#         transforms.Resize([120, 120]),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "# }\n",
    "\n",
    "# train_data = CatDogDataset(data_dir='/content/drive/MyDrive/RA/test/dataset', mode='train', transform=data_transforms['train'])\n",
    "# val_data = CatDogDataset(data_dir='/content/drive/MyDrive/RA/test/dataset', mode='val', transform=data_transforms['val'])\n",
    "# test_data = CatDogDataset(data_dir='/content/drive/MyDrive/RA/test/dataset', mode='test', transform=data_transforms['val'])\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "executionInfo": {
     "elapsed": 6146,
     "status": "error",
     "timestamp": 1677478640958,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "oYfpq_B73rzN",
    "outputId": "45ab30c6-2926-4a05-b047-e99a227efce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 2 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 3 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 4 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 5 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 6 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 7 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 8 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 9 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 10 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 11 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 12 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 13 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 14 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 15 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 16 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 17 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 18 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 19 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 20 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 21 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 22 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 23 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 24 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 25 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 26 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 27 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 28 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 29 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 30 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 31 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 32 data shape: torch.Size([40, 3, 30, 60, 60])\n",
      "Batch 33 data shape: torch.Size([40, 3, 30, 60, 60])\n"
     ]
    }
   ],
   "source": [
    "for i, (batch_data, batch_labels) in enumerate(train_loader):\n",
    "    print(f'Batch {i+1} data shape: {batch_data.shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "gsobtySa13HS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313200"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "29*60*60*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaLFWbNMlf7m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "cfKAGGi_lf7n"
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.conv 구현\n",
    "        self.conv = nn.Sequential(\n",
    "            ## 코드 시작 ##\n",
    "            torch.nn.Conv3d(3, 32, kernel_size=3),    # conv_1 해당하는 층\n",
    "            torch.nn.BatchNorm3d(32),    # batch_norm_1 해당하는 층\n",
    "            torch.nn.ReLU(),    # ReLU_1 해당하는 층\n",
    "            torch.nn.MaxPool3d(2),    # maxpool_1 해당하는 층\n",
    "            \n",
    "            torch.nn.Conv3d(32, 64, kernel_size=3),    # conv_1 해당하는 층\n",
    "            torch.nn.BatchNorm3d(64),    # batch_norm_1 해당하는 층\n",
    "            torch.nn.ReLU(),    # ReLU_1 해당하는 층\n",
    "            torch.nn.MaxPool3d(2),\n",
    "            \n",
    "            torch.nn.Conv3d(64, 128, kernel_size=3),    # conv_1 해당하는 층\n",
    "            torch.nn.BatchNorm3d(128),    # batch_norm_1 해당하는 층\n",
    "            torch.nn.ReLU(),    # ReLU_1 해당하는 층\n",
    "            torch.nn.MaxPool3d(2),\n",
    "            \n",
    "            torch.nn.Conv3d(128, 128, kernel_size=3),    # conv_1 해당하는 층\n",
    "            torch.nn.BatchNorm3d(128),    # batch_norm_1 해당하는 층\n",
    "            torch.nn.ReLU(),    # ReLU_1 해당하는 층\n",
    "            torch.nn.MaxPool3d(2)\n",
    "            ## 코드 종료 ##\n",
    "        )\n",
    "        \n",
    "        # self.fc 구현\n",
    "        ## 코드 시작 ##\n",
    "        self.fc1 = nn.Linear(4608,512)\n",
    "        self.fc2 = nn.Linear(512,12)\n",
    "        ## 코드 종료 ##\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1677478648637,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "0ur0Ie8gkkr9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.resnet as resnet\n",
    "\n",
    "class Conv3DResNet(resnet.ResNet):\n",
    "    def __init__(self, block, layers, num_classes):\n",
    "        super().__init__(block, layers, num_classes)\n",
    "\n",
    "        self.conv1 = nn.Conv3d(3, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(128)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.maxpool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 0, 0))\n",
    "\n",
    "        self.conv3 = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm3d(256)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.maxpool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 0, 0))\n",
    "\n",
    "        self.fc1 = nn.Linear(25088 , 512)\n",
    "       \n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        # Implement the rest of the layers as needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "      \n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.downsample = None\n",
    "        \n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(out_channels),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=2):\n",
    "        super(ResNet3D, self).__init__()\n",
    "        self.in_channels = 1\n",
    "\n",
    "        self.conv1 = nn.Conv3d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "IHfsaE2Zi6N7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class C3D(nn.Module):\n",
    "    def __init__(self, num_classes=12):\n",
    "        super(C3D, self).__init__()\n",
    "\n",
    "        # Define the network layers\n",
    "        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        self.conv3a = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv3b = nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        self.conv4a = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv4b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        self.conv5a = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv5b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        self.fc6 = nn.Linear(8192, 4096)\n",
    "        self.fc7 = nn.Linear(4096, 4096)\n",
    "        self.fc8 = nn.Linear(4096, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the network\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3a(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv3b(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4a(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv4b(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.conv5a(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv5b(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        x=self.fc6(x)\n",
    "        x=self.fc7(x)\n",
    "        x=self.fc8(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2j37qM2Plf7o"
   },
   "source": [
    "아래의 코드를 실행해 코드를 성공적으로 완성했는지 확인해보세요. \n",
    "\n",
    "별다른 문제가 없다면 이어서 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "error",
     "timestamp": 1677426898779,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "WAo1ZY2_lf7p",
    "outputId": "7838d283-5a57-44fd-a656-46c15089ab4d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv3d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv3d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride):\n",
    "        strides = [stride] + [1]*(blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * BasicBlock.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXlk9RPjlf7p"
   },
   "source": [
    "## 6. train, validation, test 함수 정의\n",
    "이번에는 훈련, 검증, 테스트를 진행하는 함수를 정의하겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrOVMufklf7q"
   },
   "source": [
    "### <font color='red'>[TODO] 코드 구현: 훈련 함수</font>\n",
    "\n",
    "먼저 훈련 함수입니다. 다음을 읽고 코드를 완성해 보세요.\n",
    "\n",
    "`train` 함수에 여러 인자들이 보입니다. \n",
    "\n",
    "1. 모델(`model`)에 입력 이미지(`imgs`)를 전달하고 출력 결과를 `outputs`에 저장합니다.\n",
    "2. `criterion` 은 손실함수를 담은 객체입니다. 예측 값인 `outputs`와 라벨 값인 `labels`를 통해 손실값을 계산하고 그 결과를 `loss` 변수에 저장합니다.\n",
    "3. `optimizer`은 옵티마이저입니다. 이전에 계산한 기울기를 모두 clear하고, 오차 역전파(backpropagation)를 통해 기울기를 계산하고, 옵티마이저를 통해 파라미터를 업데이트합니다. \n",
    "\n",
    "일정한 에폭마다 다음에 구현할 `validation` 함수를 통해 검증을 수행합니다. 모델 검증을 수행했을 때, 만약 검증 과정의 평균 loss가 현재까지 가장 낮다면 가장 잘 훈련된 모델로 가정하고 그때까지 학습한 모델을 저장합니다. 저장은 추후에 구현할 `save_model` 함수가 수행합니다. \n",
    "\n",
    "**tarin 함수를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1677478652534,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "um5YgnSilf7q"
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data_loader, criterion, optimizer, saved_dir, val_every, device):\n",
    "    print('Start training..')\n",
    "    best_loss = 9999999\n",
    "    \n",
    "\n",
    "\n",
    "    # define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (imgs, labels) in enumerate(data_loader):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # calculate L2 regularization loss\n",
    "\n",
    "\n",
    "            # backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate accuracy\n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            accuracy = (labels == argmax).float().mean()\n",
    "\n",
    "            if (i+1) % 3 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(data_loader), loss.item(), accuracy.item() * 100))\n",
    "\n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            avrg_loss = validation(epoch + 1, model, val_loader, criterion, device)\n",
    "            if avrg_loss < best_loss:\n",
    "                print('Best performance at epoch: {}'.format(epoch + 1))\n",
    "                print('Save model in', saved_dir)\n",
    "                best_loss = avrg_loss\n",
    "                save_model(model, saved_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1677478652942,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "SR7h3FgKtf57"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CrdYa2jlf7r"
   },
   "source": [
    "### <font color='red'>[TODO] 코드 구현: 검증 함수</font>\n",
    "\n",
    "검증 함수입니다. 다음을 읽고 코드를 완성해보세요. \n",
    "\n",
    "- 검증 과정에서는 파라미터 업데이트를 하지 않기 때문에 기울기를 계산할 필요는 없습니다. 하지만 검증 과정에서의 평균 loss를 계산하기 위해 loss는 계산해야 합니다. \n",
    "- `train` 함수와 마찬가지로 `model`에 입력 이미지를 전달하여 얻은 출력 결과를 `outputs`에 저장하고, `criterion`을 통해 loss를 계산한 뒤, 그 결과를 `loss`에 저장합니다.\n",
    "\n",
    "모델 검증 과정에서는 [model.eval()](https://pytorch.org/docs/stable/nn.html?highlight=eval#torch.nn.Module.eval)을 통해 모델을 evaluation 모드로 작동해줘야 함을 기억하시기 바랍니다. Batch normalization 과 Dropout은 훈련과 검증시에 작동하는 방식이 다르기 때문입니다. 평가가 끝난 후에는 다시 [model.train()](https://pytorch.org/docs/stable/nn.html?highlight=module%20train#torch.nn.Module.train)을 통해 train 모드로 바꿔줘야 하는 사실도 잊지 마세요.\n",
    "\n",
    "**validation 함수를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1677478653648,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "8tS_Rmswlf7r"
   },
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion, device):\n",
    "    print('Start validation #{}'.format(epoch) )\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        for i, (imgs, labels) in enumerate(data_loader):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            ## b코드 시작 ##\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs,labels)\n",
    "            ## 코드 종료 ##\n",
    "            total += imgs.size(0)\n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            correct += (labels == argmax).sum().item()\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "        avrg_loss = total_loss / cnt\n",
    "        print('Validation #{}  Accuracy: {:.2f}%  Average Loss: {:.4f}'.format(epoch, correct / total * 100, avrg_loss))\n",
    "    model.train()\n",
    "    return avrg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1677478653648,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "nHlGlRwebDKU"
   },
   "outputs": [],
   "source": [
    "# nb_classes = 9\n",
    "# confusion_matrix = np.zeros((nb_classes, nb_classes))\n",
    "# with torch.no_grad():\n",
    "#     for i, (inputs, classes) in enumerate(test_loader):\n",
    "#         inputs = inputs.to(DEVICE)\n",
    "#         classes = classes.to(DEVICE)\n",
    "#         outputs = model(inputs)\n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "#         for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "#                 confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "# plt.figure(figsize=(15,10))\n",
    "\n",
    "# class_names = list(label2class.values())\n",
    "# df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names).astype(int)\n",
    "# heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "\n",
    "# heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=15)\n",
    "# heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=15)\n",
    "# plt.ylabel('True label')\n",
    "# plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1677478653648,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "jjr079jAbOrx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MB9TJJulf7r"
   },
   "source": [
    "### <font color='red'>[TODO] 코드 구현: 테스트 함수</font>\n",
    "테스트 함수입니다. 다음을 읽고 코드를 완성해보세요.\n",
    "\n",
    "- `model`에 입력 이미지를 전달하여 얻은 출력 결과를 `outputs`에 저장합니다\n",
    "\n",
    "테스트에서는 loss를 계산할 필요가 없고, 전체 정확도를 통해 모델의 성능을 확인하면 됩니다. \n",
    "\n",
    "**test 함수를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1677478655838,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "4bsBxF-ilf7s"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "def plot_confusion_matrix(cm, labels, title='Confusion Matrix', cmap='Blues'):\n",
    "   \n",
    "     # Normalize the confusion matrix to show percentages\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm*100/54, annot=True, cmap=cmap, fmt='.2f')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def test(model, data_loader, device, num_classes):\n",
    "    print('Start test..')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        for i, (imgs, labels) in enumerate(data_loader):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            ## 코드 시작 ##\n",
    "            outputs = model(imgs)\n",
    "            ## 코드 종료 ##\n",
    "            _, argmax = torch.max(outputs, 1)    # max()를 통해 최종 출력이 가장 높은 class 선택\n",
    "            total += imgs.size(0)\n",
    "            correct += (labels == argmax).sum().item()\n",
    "            true_labels.extend(labels.cpu().numpy().tolist())\n",
    "            predicted_labels.extend(argmax.cpu().numpy().tolist())\n",
    "\n",
    "        print('Test accuracy for {} images: {:.2f}%'.format(total, correct / total * 100))\n",
    "        cm = confusion_matrix(true_labels, predicted_labels,labels=np.arange(num_classes))\n",
    "        print('Confusion Matrix:')\n",
    "        \n",
    "        plot_confusion_matrix(cm, labels=np.arange(num_classes),title='Confusion Matrix Heatmap',cmap='Blues')\n",
    "        \n",
    "        print(cm)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUQ60cQqlf7s"
   },
   "source": [
    "## 7. 모델 저장 함수 정의\n",
    "모델을 저장하는 함수입니다. 모델 저장은 [`torch.save`](https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save) 함수를 통해 할 수 있습니다. \n",
    "[`nn.Module.state_dict`](https://pytorch.org/docs/stable/nn.html?highlight=state_dict#torch.nn.Module.state_dict)를 통해 Module, 즉 우리 모델의 파라미터를 가져올 수 있습니다. 이렇게 불러온 파라미터를 **check_point** 딕셔너리에 저장합니다. 그리고 이 **check_point**를 정해준 경로에 저장하면 됩니다. \n",
    "\n",
    "[PyTorch 공식 튜토리얼](https://pytorch.org/tutorials/beginner/saving_loading_models.html)에서 모델을 저장하고 불러오는 방법에 대한 예제를 확인하실 수 있습니다. \n",
    "\n",
    "`torch.save` 는 단순히 모델의 파라미터만 저장하는 함수가 아닙니다. 어떤 파이썬 객체든 저장할 수 있습니다. 그래서 경우에 따라 **check_point** 딕셔너리에 모델의 파라미터 뿐만 아니라 다른 여러 가지 필요한 정보를 저장할 수도 있습니다. 예를 들어 총 몇 에폭동안 학습한 모델인지 그 정보도 저장할 수 있겠죠? \n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "다음을 읽고 코드를 완성해보세요.\n",
    "- torch.save를 통해 `output_path` 경로에 `check_point` 를 저장하세요.\n",
    "\n",
    "**save_model 함수를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1677478655838,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "_xcotU9Ylf7s"
   },
   "outputs": [],
   "source": [
    "def save_model(model, saved_dir, file_name='best_model.pt'):\n",
    "    os.makedirs(saved_dir, exist_ok=True)\n",
    "    check_point = {\n",
    "        'net': model.state_dict()\n",
    "    }\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    ## 코드 시작 ##\n",
    "    torch.save(check_point,output_path)\n",
    "    ## 코드 종료 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zG6LPYLglf7t"
   },
   "source": [
    "## 8. 모델 생성 및 Loss function, Optimizer 정의\n",
    "\n",
    "생성한 모델을 학습 시키기 위해서 손실함수를 정의해야 합니다. 뉴럴네트워크는 경사하강(gradient descent)방법을 이용하여 손실함수의 값을 줄이는 방향으로 파라미터를 갱신(update) 하게 됩니다. 또한 효과적인 경사하강 방법을 적용하기 위해 옵티마이져를 함께 사용할 겁니다.\n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "다음을 읽고 코드를 완성해보세요. \n",
    "1. **<5. 네트워크 설계>** 에서 정의한 SimpleCNN class를 통해 모델을 생성하고 이를 **model** 변수에 저장합니다.\n",
    "2. 분류 문제에서는 손실함수로 Cross Entropy Loss를 사용합니다. Cross Entropy Loss Function([`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html?highlight=crossentropy#torch.nn.CrossEntropyLoss))을 만들고 `criterion` 변수에 저장합니다.\n",
    "3. 이번 실습에서는 Adam 옵티마이저를 통해 파라미터를 업데이트 하겠습니다. Adam optimizer([`torch.optim.Adam`](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam))를 `optimizer` 변수에 저장합니다. **<3. 하이퍼파라미터 세팅>** 에서 정의한 `learning_rate` 를 사용하세요.\n",
    "\n",
    "`val_every`는 검증을 몇 에폭마다 진행할지 정하는 변수입니다. `saved_dir`은 모델이 저장될 디렉토리의 경로입니다. \n",
    "\n",
    "**모델을 생성하고 손실함수 및 옵티마이저를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1677478657010,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "bn0-JUUilf7t",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define the L2 regularization strength\n",
    "\n",
    "# create the optimizer and add L2 regularization to it\n",
    "\n",
    "# import torchvision.models.video as models\n",
    "import torchvision.models.video as models\n",
    "# # Load the pre-trained model\n",
    "# model = models.r3d_18(pretrained=True)  # 위의 설명 1. 을 참고하여 None을 채우세요.  \n",
    "# model = model.to(device)\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Load the pretrained SlowOnly model\n",
    "\n",
    "\n",
    "# Print the model architecture\n",
    "\n",
    "model = models.mc3_18(pretrained=True)  # 위의 설명 1. 을 참고하여 None을 채우세요.  \n",
    "model = model.to(device)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "## 코드 시작 ##\n",
    "model.fc = nn.Linear(num_ftrs,8) \n",
    "\n",
    "model = model.to(device)\n",
    "# # Load the pre-trained model\n",
    "# model = ResNet18()\n",
    "#  # 위의 설명 1. 을 참고하여 None을 채우세요.  \n",
    "# model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()      # 위의 설명 2. 를 참고하여 None을 채우세요.\n",
    "optimizer = optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)    # 위의 설명 3. 을 참고하여 None을 채우세요.\n",
    "## 코드 종료 ##\n",
    "CUDA_LAUNCH_BLOCKING=1.\n",
    "model = model.to(device)\n",
    "val_every = 1\n",
    "saved_dir = './saved/SimpleCNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1949,
     "status": "ok",
     "timestamp": 1677478664818,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "u4tgHsQjTIHf",
    "outputId": "aead3b0c-1c08-4b5f-de6f-1e7c73842eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1       [-1, 64, 30, 30, 30]          28,224\n",
      "       BatchNorm3d-2       [-1, 64, 30, 30, 30]             128\n",
      "              ReLU-3       [-1, 64, 30, 30, 30]               0\n",
      "      Conv3DSimple-4       [-1, 64, 30, 30, 30]         110,592\n",
      "       BatchNorm3d-5       [-1, 64, 30, 30, 30]             128\n",
      "              ReLU-6       [-1, 64, 30, 30, 30]               0\n",
      "      Conv3DSimple-7       [-1, 64, 30, 30, 30]         110,592\n",
      "       BatchNorm3d-8       [-1, 64, 30, 30, 30]             128\n",
      "              ReLU-9       [-1, 64, 30, 30, 30]               0\n",
      "       BasicBlock-10       [-1, 64, 30, 30, 30]               0\n",
      "     Conv3DSimple-11       [-1, 64, 30, 30, 30]         110,592\n",
      "      BatchNorm3d-12       [-1, 64, 30, 30, 30]             128\n",
      "             ReLU-13       [-1, 64, 30, 30, 30]               0\n",
      "     Conv3DSimple-14       [-1, 64, 30, 30, 30]         110,592\n",
      "      BatchNorm3d-15       [-1, 64, 30, 30, 30]             128\n",
      "             ReLU-16       [-1, 64, 30, 30, 30]               0\n",
      "       BasicBlock-17       [-1, 64, 30, 30, 30]               0\n",
      " Conv3DNoTemporal-18      [-1, 128, 30, 15, 15]          73,728\n",
      "      BatchNorm3d-19      [-1, 128, 30, 15, 15]             256\n",
      "             ReLU-20      [-1, 128, 30, 15, 15]               0\n",
      " Conv3DNoTemporal-21      [-1, 128, 30, 15, 15]         147,456\n",
      "      BatchNorm3d-22      [-1, 128, 30, 15, 15]             256\n",
      "           Conv3d-23      [-1, 128, 30, 15, 15]           8,192\n",
      "      BatchNorm3d-24      [-1, 128, 30, 15, 15]             256\n",
      "             ReLU-25      [-1, 128, 30, 15, 15]               0\n",
      "       BasicBlock-26      [-1, 128, 30, 15, 15]               0\n",
      " Conv3DNoTemporal-27      [-1, 128, 30, 15, 15]         147,456\n",
      "      BatchNorm3d-28      [-1, 128, 30, 15, 15]             256\n",
      "             ReLU-29      [-1, 128, 30, 15, 15]               0\n",
      " Conv3DNoTemporal-30      [-1, 128, 30, 15, 15]         147,456\n",
      "      BatchNorm3d-31      [-1, 128, 30, 15, 15]             256\n",
      "             ReLU-32      [-1, 128, 30, 15, 15]               0\n",
      "       BasicBlock-33      [-1, 128, 30, 15, 15]               0\n",
      " Conv3DNoTemporal-34        [-1, 256, 30, 8, 8]         294,912\n",
      "      BatchNorm3d-35        [-1, 256, 30, 8, 8]             512\n",
      "             ReLU-36        [-1, 256, 30, 8, 8]               0\n",
      " Conv3DNoTemporal-37        [-1, 256, 30, 8, 8]         589,824\n",
      "      BatchNorm3d-38        [-1, 256, 30, 8, 8]             512\n",
      "           Conv3d-39        [-1, 256, 30, 8, 8]          32,768\n",
      "      BatchNorm3d-40        [-1, 256, 30, 8, 8]             512\n",
      "             ReLU-41        [-1, 256, 30, 8, 8]               0\n",
      "       BasicBlock-42        [-1, 256, 30, 8, 8]               0\n",
      " Conv3DNoTemporal-43        [-1, 256, 30, 8, 8]         589,824\n",
      "      BatchNorm3d-44        [-1, 256, 30, 8, 8]             512\n",
      "             ReLU-45        [-1, 256, 30, 8, 8]               0\n",
      " Conv3DNoTemporal-46        [-1, 256, 30, 8, 8]         589,824\n",
      "      BatchNorm3d-47        [-1, 256, 30, 8, 8]             512\n",
      "             ReLU-48        [-1, 256, 30, 8, 8]               0\n",
      "       BasicBlock-49        [-1, 256, 30, 8, 8]               0\n",
      " Conv3DNoTemporal-50        [-1, 512, 30, 4, 4]       1,179,648\n",
      "      BatchNorm3d-51        [-1, 512, 30, 4, 4]           1,024\n",
      "             ReLU-52        [-1, 512, 30, 4, 4]               0\n",
      " Conv3DNoTemporal-53        [-1, 512, 30, 4, 4]       2,359,296\n",
      "      BatchNorm3d-54        [-1, 512, 30, 4, 4]           1,024\n",
      "           Conv3d-55        [-1, 512, 30, 4, 4]         131,072\n",
      "      BatchNorm3d-56        [-1, 512, 30, 4, 4]           1,024\n",
      "             ReLU-57        [-1, 512, 30, 4, 4]               0\n",
      "       BasicBlock-58        [-1, 512, 30, 4, 4]               0\n",
      " Conv3DNoTemporal-59        [-1, 512, 30, 4, 4]       2,359,296\n",
      "      BatchNorm3d-60        [-1, 512, 30, 4, 4]           1,024\n",
      "             ReLU-61        [-1, 512, 30, 4, 4]               0\n",
      " Conv3DNoTemporal-62        [-1, 512, 30, 4, 4]       2,359,296\n",
      "      BatchNorm3d-63        [-1, 512, 30, 4, 4]           1,024\n",
      "             ReLU-64        [-1, 512, 30, 4, 4]               0\n",
      "       BasicBlock-65        [-1, 512, 30, 4, 4]               0\n",
      "AdaptiveAvgPool3d-66         [-1, 512, 1, 1, 1]               0\n",
      "           Linear-67                    [-1, 8]           4,104\n",
      "================================================================\n",
      "Total params: 11,494,344\n",
      "Trainable params: 11,494,344\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.24\n",
      "Forward/backward pass size (MB): 419.59\n",
      "Params size (MB): 43.85\n",
      "Estimated Total Size (MB): 464.68\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,input_size=(3,30,60,60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6MCTeMMlf7t"
   },
   "source": [
    "아래의 코드를 실행해 코드를 성공적으로 완성했는지 확인해보세요. \n",
    "\n",
    "별다른 문제가 없다면 이어서 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "error",
     "timestamp": 1677424221973,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "OeOF3PBKlf7u",
    "outputId": "cf02bc34-5e8b-4512-b15e-a6a87d1a7bd1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EZbgqc7lf7u"
   },
   "source": [
    "\n",
    "## 9. Training\n",
    "\n",
    "**<6. train, validation, test 함수 정의>** 에서 작성한 `train` 함수를 통해 학습을 진행합니다. 네트워크의 규모가 큰 편이 아니지만, CPU를 통해 학습되기 때문에 시간이 조금 필요합니다. 컴퓨터 성능에 따라 20~30분의 시간이 소요될 수 있습니다. 시간 여유가 없는 분들은 모델 학습이 적당히 진행된다는 정도만 확인하고 다음 단계로 넘어가셔도 됩니다. \n",
    "\n",
    "만약 어느정도 기다렸음에도 학습 accuracy가 50%를 넘는 양상을 보이지 않는다면 구현한 코드에 문제가 있을 수 있습니다. 이러한 경우에는 구현한 train 함수를 다시 한 번 확인하시기 바랍니다. \n",
    "\n",
    "또한, 모델 저장 코드를 제대로 구현했다면 첫 에폭 학습후에 ./saved/SimpleCNN 경로에 best_model.pt 파일이 저장되어 있어야 합니다. 만약에 파일이 존재하지 않는다면 모델 저장 코드를 다시 확인하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "executionInfo": {
     "elapsed": 10441,
     "status": "error",
     "timestamp": 1677478683665,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "gka-_OZQlf7u",
    "outputId": "14fa5983-fab2-4afd-90ac-b7b4770d699a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training..\n",
      "Epoch [1/20], Step [3/33], Loss: 1.9329, Accuracy: 30.00%\n",
      "Epoch [1/20], Step [6/33], Loss: 1.4442, Accuracy: 62.50%\n",
      "Epoch [1/20], Step [9/33], Loss: 1.1539, Accuracy: 65.00%\n",
      "Epoch [1/20], Step [12/33], Loss: 1.0521, Accuracy: 72.50%\n",
      "Epoch [1/20], Step [15/33], Loss: 0.7717, Accuracy: 77.50%\n",
      "Epoch [1/20], Step [18/33], Loss: 0.8009, Accuracy: 75.00%\n",
      "Epoch [1/20], Step [21/33], Loss: 0.8437, Accuracy: 70.00%\n",
      "Epoch [1/20], Step [24/33], Loss: 0.4927, Accuracy: 85.00%\n",
      "Epoch [1/20], Step [27/33], Loss: 0.8430, Accuracy: 72.50%\n",
      "Epoch [1/20], Step [30/33], Loss: 0.6014, Accuracy: 82.50%\n",
      "Epoch [1/20], Step [33/33], Loss: 0.7712, Accuracy: 70.00%\n",
      "Start validation #1\n",
      "Validation #1  Accuracy: 77.75%  Average Loss: 0.6883\n",
      "Best performance at epoch: 1\n",
      "Save model in ./saved/SimpleCNN\n",
      "Epoch [2/20], Step [3/33], Loss: 0.4773, Accuracy: 85.00%\n",
      "Epoch [2/20], Step [6/33], Loss: 0.2838, Accuracy: 90.00%\n",
      "Epoch [2/20], Step [9/33], Loss: 0.4954, Accuracy: 80.00%\n",
      "Epoch [2/20], Step [12/33], Loss: 0.3906, Accuracy: 90.00%\n",
      "Epoch [2/20], Step [15/33], Loss: 0.4496, Accuracy: 87.50%\n",
      "Epoch [2/20], Step [18/33], Loss: 0.3043, Accuracy: 90.00%\n",
      "Epoch [2/20], Step [21/33], Loss: 0.7878, Accuracy: 75.00%\n",
      "Epoch [2/20], Step [24/33], Loss: 0.4066, Accuracy: 87.50%\n",
      "Epoch [2/20], Step [27/33], Loss: 0.5382, Accuracy: 80.00%\n",
      "Epoch [2/20], Step [30/33], Loss: 0.2825, Accuracy: 92.50%\n",
      "Epoch [2/20], Step [33/33], Loss: 0.6534, Accuracy: 77.50%\n",
      "Start validation #2\n",
      "Validation #2  Accuracy: 80.50%  Average Loss: 0.6201\n",
      "Best performance at epoch: 2\n",
      "Save model in ./saved/SimpleCNN\n",
      "Epoch [3/20], Step [3/33], Loss: 0.1853, Accuracy: 97.50%\n",
      "Epoch [3/20], Step [6/33], Loss: 0.1739, Accuracy: 97.50%\n",
      "Epoch [3/20], Step [9/33], Loss: 0.2204, Accuracy: 95.00%\n",
      "Epoch [3/20], Step [12/33], Loss: 0.2738, Accuracy: 85.00%\n",
      "Epoch [3/20], Step [15/33], Loss: 0.1287, Accuracy: 100.00%\n",
      "Epoch [3/20], Step [18/33], Loss: 0.2614, Accuracy: 97.50%\n",
      "Epoch [3/20], Step [21/33], Loss: 0.1268, Accuracy: 97.50%\n",
      "Epoch [3/20], Step [24/33], Loss: 0.0856, Accuracy: 97.50%\n",
      "Epoch [3/20], Step [27/33], Loss: 0.1715, Accuracy: 92.50%\n",
      "Epoch [3/20], Step [30/33], Loss: 0.2612, Accuracy: 92.50%\n",
      "Epoch [3/20], Step [33/33], Loss: 0.2645, Accuracy: 87.50%\n",
      "Start validation #3\n",
      "Validation #3  Accuracy: 83.25%  Average Loss: 0.5933\n",
      "Best performance at epoch: 3\n",
      "Save model in ./saved/SimpleCNN\n",
      "Epoch [4/20], Step [3/33], Loss: 0.0576, Accuracy: 100.00%\n",
      "Epoch [4/20], Step [6/33], Loss: 0.1839, Accuracy: 95.00%\n",
      "Epoch [4/20], Step [9/33], Loss: 0.1311, Accuracy: 95.00%\n",
      "Epoch [4/20], Step [12/33], Loss: 0.0685, Accuracy: 100.00%\n",
      "Epoch [4/20], Step [15/33], Loss: 0.1161, Accuracy: 95.00%\n",
      "Epoch [4/20], Step [18/33], Loss: 0.1635, Accuracy: 92.50%\n",
      "Epoch [4/20], Step [21/33], Loss: 0.1393, Accuracy: 95.00%\n",
      "Epoch [4/20], Step [24/33], Loss: 0.0906, Accuracy: 97.50%\n",
      "Epoch [4/20], Step [27/33], Loss: 0.1517, Accuracy: 95.00%\n",
      "Epoch [4/20], Step [30/33], Loss: 0.1300, Accuracy: 95.00%\n",
      "Epoch [4/20], Step [33/33], Loss: 0.2271, Accuracy: 92.50%\n",
      "Start validation #4\n",
      "Validation #4  Accuracy: 77.75%  Average Loss: 0.7819\n",
      "Epoch [5/20], Step [3/33], Loss: 0.0598, Accuracy: 100.00%\n",
      "Epoch [5/20], Step [6/33], Loss: 0.0291, Accuracy: 100.00%\n",
      "Epoch [5/20], Step [9/33], Loss: 0.1303, Accuracy: 97.50%\n",
      "Epoch [5/20], Step [12/33], Loss: 0.1131, Accuracy: 97.50%\n",
      "Epoch [5/20], Step [15/33], Loss: 0.0916, Accuracy: 97.50%\n",
      "Epoch [5/20], Step [18/33], Loss: 0.0594, Accuracy: 97.50%\n",
      "Epoch [5/20], Step [21/33], Loss: 0.0494, Accuracy: 100.00%\n",
      "Epoch [5/20], Step [24/33], Loss: 0.0741, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "train(3, model, train_loader, criterion, optimizer, saved_dir, val_every, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "id": "V_a2N39r7aGn"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2044285001.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [354]\u001b[0;36m\u001b[0m\n\u001b[0;31m    elif basename.startswith(\"animation.9.\"):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "        elif basename.startswith(\"animation.9.\"):\n",
    "#             label = 6 #\"발차기\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmnw8nuElf7u"
   },
   "source": [
    "## 10. 저장된 모델 불러오기 및 test\n",
    "학습한 모델의 성능을 테스트합니다. 저장한 모델 파일을 [`torch.load`](https://pytorch.org/docs/stable/torch.html?highlight=load#torch.load)를 통해 불러옵니다. 위에서 학습을 끝까지 진행하지 않았다면, 아래의 주석 처리된 부분을 주석 해제하면, 제공해드린 미리 학습시킨 모델을 불러올 수 있습니다. \n",
    "\n",
    "이렇게 불러오면 우리가 얻게 되는 건 아까 저장한 **check_point** 딕셔너리입니다. 딕셔너리에 저장한 모델의 파라미터는 **'net'** key에 저장해두었습니다. 이를 불러와 **state_dict**에 저장합니다. 이렇게 불러온 모델의 파라미터를 모델에 실제로 로드하기 위해서는 [`nn.Module.load_state_dict`](https://pytorch.org/docs/stable/torch.html?highlight=load#torch.load)를 사용하면 됩니다.  \n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "다음을 읽고 코드를 완성해보세요.\n",
    "\n",
    "1. `model_path`의 경로에 있는 모델 파일을 로드하여, 이를 `check_point` 변수에 저장합니다. 또한, 미리저장된 모델이 GPU로 학습했는데 CPU 로 불러올 경우, 파이토치에서 모델을 불러오는 함수에 `map_location` 인자에 `device` 정보를 전달해야 적용됩니다. (즉, 함수에 `map_location=device` 가 되어야 합니다.) `device` 변수는 **<1. Package load>** 에서 이미 선언했습니다.\n",
    "2. `check_point` 딕셔너리에 접근하여 모델의 파라미터를 `state_dict` 변수에 저장합니다. 접근을 위한 딕셔너리의 키값은 'net' 입니다.\n",
    "3. `state_dict`의 파라미터들을 새로 선언한 모델(`model`)에 로드합니다.\n",
    "\n",
    "**새로 선언한 `model` 에 저장된 파라미터를 불러오는 코드를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1677427501237,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "MyAfHjw7lf7v",
    "outputId": "fe882cd4-4ef7-42b5-8090-632a57f746fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './saved/SimpleCNN/best_model.pt'\n",
    "#model_path = './saved/pretrained/SimpleCNN/best_model.pt' # 모델 학습을 끝까지 진행하지 않은 경우에 사용\n",
    "model = model.to(device)   # 아래의 모델 불러오기를 정확히 구현했는지 확인하기 위해 새로 모델을 선언하여 학습 이전 상태로 초기화\n",
    "\n",
    "## 코드 시작 ##\n",
    "checkpoint = torch.load(model_path,map_location=device)   # 위의 설명 1. 을 참고하여 None을 채우세요.\n",
    "state_dict    = checkpoint['net']   # 위의 설명 2. 를 참고하여 None을 채우세요.\n",
    "model.load_state_dict(state_dict)                  # 위의 설명 3. 을 참고하여 None을 채우세요.\n",
    "## 코드 종료 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFdV6vRPlf7v"
   },
   "source": [
    "마지막으로 모델의 성능을 테스트합니다. 75% 내외의 성능이 나온다면 학습 및 모델 불러오기가 성공적으로 진행된 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "executionInfo": {
     "elapsed": 5437,
     "status": "ok",
     "timestamp": 1677427510254,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "grxs2T-Tlf7v",
    "outputId": "c9abc4c2-e2a9-421f-b78e-838fe9f0a3bc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start test..\n",
      "Test accuracy for 480 images: 81.04%\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhAAAAGoCAYAAAAXcdwMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABtvUlEQVR4nO3dd3gUVdvH8e+dTSChhYApCAGkiEoRERCR3nuvIoogQVF80IcHBVQUaXZ9raCiIipNEFBAlCKhN2kKinSEFHogCSSb8/6xS0xICNmY3c2E++O1l9mZOTO/HWZ37z1ndkeMMSillFJKucLH2wGUUkopZT1aQCillFLKZVpAKKWUUsplWkAopZRSymVaQCillFLKZVpAKKWUUsplWkCoG5aIBIjIIhE5JyJz/sV6+onIstzM5g0iskREHvJ2DqWUNWgBofI8EblfRLaIyAUROeF8o2uQC6vuAYQCJY0xPXO6EmPMV8aYVrmQJx0RaSIiRkTmXTX9Tuf0Vdlcz4siMuN6yxlj2hpjvshBzgEisiaT6YdEpIWr68tkPdnKr5TyLC0gVJ4mIk8DbwMTcbzZlwU+ADrnwurLAX8aY5JzYV3uEgvUF5GSaaY9BPyZWxsQB30tUEq5RF80VJ4lIoHAOOBxY8w8Y8xFY0ySMWaRMeZ/zmUKisjbInLceXtbRAo65zURkWMi8l8RiXH2XjzsnPcS8ALQ29mzMejqT7oiUt75Sd/XeX+AiBwQkTgROSgi/dJMX5OmXX0R2ewcGtksIvXTzFslIi+LyFrnepaJyE1Z7IbLwHdAH2d7G9AL+OqqffWOiBwVkfMislVEGjqntwFGp3mcO9LkmCAia4F4oIJz2iPO+R+KyNw0639FRJaLiGT33+9qIjJQRPaIyBkR+VFEyv3L/ONFZJ1z+iIRKSkiXznXsVlEyl9v/c55L4rIXBGZ5fw32SYid+b0cSp1o9ACQuVl9wL+wPwslhkD1ANqAncCdYHn0swPAwKB0sAg4H0RCTLGjMXRqzHLGFPEGPNpVkFEpDDwf0BbY0xRoD6wPZPlSgA/OJctCbwJ/HBVD8L9wMNACFAAGJHVtoHpwIPOv1sDvwHHr1pmM459UAL4GpgjIv7GmKVXPc60b4z9gQigKHD4qvX9F6jhLI4a4th3D5kc/va9iHTBUQh0A4KBSOCbf5m/j/MxlAYqAuuBz5zr2AOMvd7608zvDMxJM/87EfHLyWNV6kahBYTKy0oCJ68zxNAPGGeMiTHGxAIv4XhTuSLJOT/JGLMYuABUyWGeFKCaiAQYY04YY37LZJn2wD5jzJfGmGRjzDfAXqBjmmU+M8b8aYxJAGbjeGO7JmPMOqCEiFTBUUhMz2SZGcaYU85tvgEU5PqP83NjzG/ONklXrS8eeABHATQDGGaMOZbFuuqJyNm0NxzDTVcMASYZY/Y4/z0nAjWv9ELkMP9nxpj9xphzwBJgvzHmZ+f65wB3ubB/thpj5jr3w5s4Ctd619m+Ujc0LSBUXnYKuOnKEMI13Ez6T8+HndNS13FVARIPFHE1iDHmItAbeBQ4ISI/iMht2chzJVPpNPejcpDnS+AJoCmZ9Mg4h2n2OIdNzuLodclqaATgaFYzjTGbgAOA4Ch0srLBGFM87Q04kmZ+OeCdNMXFaed6S/+L/NFp/k7I5H7qfs3G+lP3hTEmBThG+uNIKXUVLSBUXrYeSAS6ZLHMcRxvTleUJWP3fnZdBAqluR+WdqYx5kdjTEugFI5ehY+zkedKpr9zmOmKL4GhwGJn70Aq5xDDMzjOjQhyvnmfw/EGDXCtYYcshyNE5HEcn9SPAyNznNzhKDDkqiIjwBiz7l/kz5ZsrB8gPM3yPkAZcn4cKXVD0AJC5VnOrukXcJy30EVEComIn4i0FZFXnYt9AzwnIsHOkxFfwNHlnhPbgUYiUlYcJ3COujJDREJFpJPzXIhLOIZC7JmsYzFwqzi+euorIr2BO4Dvc5gJAGPMQaAxjnM+rlYUSMbxjQ1fEXkBKJZmfjRQXlz4poWI3AqMxzGM0R8YKSI1c5YegI+AUSJS1bn+QBG58tXZXM9/leutH+BuEenm7O0ajuPfeEMOt6fUDUELCJWnGWPeBJ7GcWJkLI5Psk/g+GYCON7ktgA7gV3ANue0nGzrJ2CWc11bSf+m74PjxMLjOLrfG+PoEbh6HaeADs5lT+H45N7BGHMyJ5muWvcaY0xmn4p/xHEOwJ84hksSST88ceVHsk6JyLbrbcf5JjoDeMUYs8MYsw/HCZBfivMbLjnIPh94BZgpIueB3UBbd+TPxPXWD7AAxxDVGRwFU7erzwtRSqUnOTypWiml8gUReRGoZIx5wNtZlLIS7YFQSimllMu0gFBKKaWUy3QIQymllFIuy+r79f+K8zvynXF8z9vgOPlsoTFmj7u2qZRSSinPcEsPhIg8A/QFZuL4QRZwfK+6DzDTGDP5Gu0icPy0Lr7lWt7tG1Ij17N5wtEVmT68PM/fz+btCDccX1uOLy2hlPIAf1888iQNuOuJXHkzTvj1PY+9qLirB2IQUPXqr0GJyJs4fsc/03dYY8xUYCpAQN0ROrailFJK5VHuKiBSyPwnfUs55ymllFLqihz/Tpr3uKuAGA4sF5F9/PODLWWBSjh+BEgppZRSV4j1hjPdUkAYY5Y6fwq3Lo6TKAXHuRCbjTGZ/fyvUkoppSzEbd/CcF7RTn9LXimllLoeHcJQSimllMt0CEMppZRSLrNgD4T1EiullFLK67QHQimllPI2HcJQSimllMt0CEMppZRSNwLtgVBKKaW8TYcwlFJKKeUyHcJQSiml1I1AeyCUUkopb7PgEEa+6IF4vHcDtnwzgq0zR/BEn4YABBUL4Pt3I9g19xm+fzeC4kUDMrSrXDaYDTOeSr1Frxif2v6K4f0ak7DpdUoGFnL74+jeoSX9e3Xhob7dGPhArwzzt23ZRKtG9/BQ32481Lcb06Z+kDovLu48Y0YOp2+3DtzfvSO7d253e16Al14YTYvG9enVtWOm8+Pi4hj+xKP06dGZnl07sPC7b1PndWjTjF7dOtK3Zxce6NPdI3nTsnL2tZGr6dS+NR3atOTTj6dmmG+MYfLE8XRo05IeXTuy5/ffst3W3aya3aq5s7P9vJrdqrlzRHxy5+ZBlu+BuKNCGA93qUfDAe9wOdnOwnceYcnaPQzscg+rNu/j9ekrGfFgU0Y81Izn3vshXdt9R2Kp98BbAPj4CPt/eJ6Fq3anzi8TEkize27lyIkzHns87075jOJBQdecf+ddd/PaOx9kmP72a5O4594GTHj1bZKSLpOYmOjOmKk6dupKrz79GDvm2Uznz5n5FRUqVuLt9z7izOnTdOvUlrbtO+LnVwCAKZ9OJyiLx+tOVs1ut9uZOGEcUz7+jNDQUO7v3YMmTZtRsVKl1GXWRK7myOFDLFqyjF07dzB+3It8NXNOttpq9vyT28rZrZo7x7QHwvNuuyWETbsPk3ApCbs9hchtB+jcpBodGlVlxg9bAJjxwxY6Nq6a5Xqa1qnMwWOnOBL1T7Hw6lOdGfPu9xhj3PoY/q2LFy6w49etdOzi+CTs51eAokWLeWTbtWrXITAw8NoLiHDx4kWMMcTHx1MsMBCbLW/UrVbNvnvXTsLDy1EmPBy/AgVo0649q1YuT7fMyhXL6dipCyJCjTtrEhd3ntjYmGy11ez5J7eVs1s1943E8gXEb/ujaHBXBUoEFiKgoB9t7ruNMqHFCSlRlKhTcQBEnYojOKhIluvp2bIms5dtT73fvuEdHI89x659J9wZPx0R4anHBzOwX08WzJud6TK7d23noT5d+e+wIRzY/xcAf/99lOJBQUx4cQwD7u/OpHEvkJAQ77HcWendtx8HD+6ndfNG9O7eiRHPjMbHx3HYCcLjQwbRr3c35s2d5eWkGeXV7DHR0YSVCku9HxIaSnR0dPplYqIJDftnmdDQMGKio7PV1p2smt2qucG62a2aO8d0CMPz/jgUwxvTV/L9uxFcTLjMzn0nSLanuLQOP18b7RtV5YUPFgMQUNCPZx5uQYdhnh03+3DaDIKDQzhz+hTDhz5CufIVqFmrdur8Krfdwbff/0ShQoVZt2Y1o/47jFnfLcFut/Pn3j089b8xVK1eg7dfm8SXn31CxNAnPZo/M+vXrqFKlduZ8skXHDt6hKERA7mrVm2KFCnCtOlfExwSyulTpxg6ZCDly1egVu063o6cKq9mN2TsEZOruz8z6TUTkey1dSOrZrdqbrBudqvmzjH9Gqd3fLFwE/UffJuWQz7gzLl4/jpykpjTcYSVLApAWMmixJ65cM32revfxva9x4g57VimQpmSlLu5BJu+epq9342mdEgg6798ilDn+twlODgEgKASJWnUtAW/796Vbn7hIkUoVKgwAPUbNCI5OZmzZ84QEhJKcEgoVavXAKBJi1b8uXePW7Nm18IF82nWvCUiQnjZctxcugyHDh4AIDgkFIASJUvStFkLdu/e6c2oGeTV7KGhYUSdiEq9HxMdTUhISLplQkLDiI76Z5no6CiCQ0Ky1dadrJrdqrnButmtmvtGki8KiCvDE+GhxenctDqzl/3KD6t/54H2jk/vD7Svzferf7tm+16t0g9f/LY/inJtXuS2LhO5rctE/o45x7393yLaOSTiDgkJ8Vy8eDH1700b1lHhqhN+Tp2MTT0f4/fdOzEpKQQWL07Jm4IJCQ3j8KGDAGzdtIHyFSq6LasrwsJKsWnjegBOnTrJ4cMHKV0mnIT4eC5edBRsCfHxbFi/lkqVbvVm1Azyavaq1apz5Mghjh07StLlyyxd/AONmzZLt0yTps1YtPA7jDHs3LGdIkWKEhwckq22mj3/5LZydqvmzjEfyZ2bB1l+CAPgm1cepESxwiTZ7Qx/bR5n4xJ4ffoKZkzsz0Od6nI0+iz9Rk0HoNRNxfhgTE+6PvUp4BiuaHbPrTwx6dusNuF2p0+dYvQIx5BDst1OqzbtqVe/IfOd4+tde/Rm5fJlzJ87C1+bjQIF/Xlp0uup3XJPjRzNS889Q3JSEjeXLsPoF8d7JPfokU+zZctmzp49Q9sWjRkydBjJyckA9OjVh8FDHmPs86Po1a0jGHhy+AiCgoI4duwoI4Y/ATjOtm7TtgP1GzTMalOa3cnX15dRY17gsYhHSEmx06VrdypVqszsWd8A0Kt3Xxo2asya1b/QoW1L/P0DGDd+YpZtNXv+zG3l7FbNnWMWHMKQvPoNg4C6I/JmsGw4umKytyPkiL+fzdsRbji+tjw+LqvUDc7fF488SQOaT8yV97yE5aM99qJivZJHKaWUUl6XL4YwlFJKKUuz4BCGFhBKKaWUt+X1r5lmwnolj1JKKaW8TnsglFJKKW/TIQyllFJKucyCQxhaQCillFLeZsEeCOslVkoppZTXaQ+EUkop5W06hKGUUkopl+kQhlJKKaVuBNoDoZRSSnmbDmEopZRSymU6hKGUUkqpG4H2QCillFLeZsEeCC0glFJKKW/TcyCuT0QeNsZ8do15EUAEwOQ336ffgEc8mi23hHd7y9sRcuTEgqe9HSHH/P1s3o5ww0m2G29HyBFfm/VeqNUNQHsgsuUlINMCwhgzFZgKcOzMZWu+OimllFI3ALcUECKy81qzgFB3bFMppZSyLB3CSBUKtAbOXDVdgHVu2qZSSillTTqEkep7oIgxZvvVM0RklZu2qZRSSikPcUsBYYwZlMW8+92xTaWUUsqyPDiEISJPAY8ABtgFPAwUAmYB5YFDQC9jzNWjCOlYr89EKaWUymdEJFdu2dhOaeBJoLYxphpgA/oAzwLLjTGVgeXO+1nSAkIppZTyMk8VEE6+QICI+OLoeTgOdAa+cM7/AuhyvZVoAaGUUkrdIIwxfwOvA0eAE8A5Y8wyINQYc8K5zAkg5Hrr0gJCKaWU8jbJnZuIRIjIljS3iHSbEQnC0dtwC3AzUFhEHshJZP0pa6WUUsrLXBh+yFLaH2S8hhbAQWNMrHO784D6QLSIlDLGnBCRUkDM9balPRBKKaXUjeMIUE9EComjamkO7AEWAg85l3kIWHC9FWkPhFJKKeVludUDcT3GmI0iMhfYBiQDv+LosSgCzBaRQTiKjJ7XW5cWEEoppZSXeaqAADDGjAXGXjX5Eo7eiGzTIQyllFJKuUx7IJRSSikv82QPRG7Jdz0Q386awaD7uzKwbxe+nfllhvk/L/2eR/p145F+3Rg2+AH27/sjdd6FuPO8OOppBvTuyMO9O/Hbru1uzTqs291snfowW6YO4ItRHSjoZ2NM//rs//pRNnz4EBs+fIjWdW7JtO1HT7fh8OyhbJk6IN306hWCWfV2PzZPGcDccV0pWqiAWx/DpUuXeLhfb/r16kqfbh2Z+sG7GZb58vNPeaBXVx7o1ZW+3Ttxb61qnDt3FoD1ayPp2bkd3Tu25otpH7s169XWRq6mU/vWdGjTkk8/znjSsjGGyRPH06FNS3p07cie33/Ldlt3s2r2l14YTYvG9enVtWOm8+Pi4hj+xKP06dGZnl07sPC7b1PndWjTjF7dOtK3Zxce6NPdU5FTWXWfZ2f7eTW7VXPnSC59jdOT8lUPxMH9+1i84Fven/Y1fr5+PDv8Ue6p34gyZculLlPq5jK89eFnFC0WyMZ1kbw56SXen/Y1AO+99Qp16t3Hi5PeJCkpiUuJCW7LenPJIgztUou7HvmMxMvJzBjTkZ5NbgPg3XlbeXvu5izbf/nTbj5auI1PRrZLN/3Dp1rz7NRVrNl1jAdbV+OpnnUY98Vatz2OAgUK8P7H0yhUqDDJSUlEPPwA9zZoRPUad6Yu03/AIPoPcFweJfKXlXwzYzqBgcWx2+28Nmk87370CSGhoQzo15uGjZtSoWIlt+W9wm63M3HCOKZ8/BmhoaHc37sHTZo2o2Klf7a9JnI1Rw4fYtGSZezauYPx417kq5lzstVWs2euY6eu9OrTj7FjMv+V3Dkzv6JCxUq8/d5HnDl9mm6d2tK2fUf8/ByF8JRPpxMUFOSRrGlZeZ9bNbtVc+eU9kB42ZFDB7i9ag38/QOw+fpSo1Zt1vyyPN0yVWvUpGixQADuqFaD2NhoAC5evMCuX7fSrlM3APz8/ChStJhb8/rafAgo6IvNRwgo6MeJ0xez3XbtrmOcjkvMML1ymRKs2XUMgBXbDtOlwa25ljczIkKhQoUBSE5OJjk5OctrwixbsphWbRxFz++7d1EmvCyly4Tj51eAlq3bsnrVCrfmvWL3rp2Eh5ejTHg4fgUK0KZde1atTH+srFyxnI6duiAi1LizJnFx54mNjclWW82euVq16xAYGHjtBUS4ePEixhji4+MpFhiIzeb9zzlW3udWzW7V3DeSfFVAlK9QmZ3bt3Lu3FkSExPYuC6S2Oioay6/ZNF86tZrAMCJv48RGBTEqy8/x5AHe/L6hLEkJMS7LevxUxd4e85m/pwxhIMzh3I+/hLLtx4C4NFOd7HpowF89HQbihcp6NJ6fz90kg73Oqrsbo2qUCbYvUUQOD4pPNCrK22aNaBuvfpUq35npsslJiSwYV0kTVu0BCAmJprQsLDU+SGhYcTGXPe3S3JFTHQ0YaXSbjuU6Ojo9MtclS80NIyY6OhstXUnK2e/nt59+3Hw4H5aN29E7+6dGPHMaHx8HC9TgvD4kEH0692NeXNneTSXlfe5VbNbNXdOefhaGLkiXxUQ5W6pQJ/+Axk5LIJnhz9KxcpVsPnaMl32162bWLJwHoOfeApwvAnu+2MPnbr1Zsr0OfgHBDBz+qduy1q8SEE61K/E7Q9OpULfDyns70ef5nfw8aLt3DHgY+557HOiTl9gckRTl9Y75M2lDOl0F2vf70+RgAJcTra76RH8w2azMWP2fBb9uJLfdu9i/1/7Ml0ucvUqatSsRWBgcccEYzIs46nj35DZtq/aeKb5JHtt3cjK2a9n/do1VKlyOz8uX803c+bz6sSXuXDhAgDTpn/N17Pn8e4HHzN75tds25L1MF9usvI+t2p2q+bOKS0g8oB2nboxZfps3v7oC4oWC6R0mXIZltm/7w/emDiWca/9X+qbWXBIKMHBodxerQYAjZq1ZN8fe9yWs9ld5TgUdY6T5xJItqfw3Zp91LvjZmLOxpOSYjAGpi3ZSe3bwq6/sjT+PHqajqPmcN/jXzJ75R4OHj/rngeQiaLFinF37TqsXxuZ6fyflv4zfAGOHofoqH96iGKio7gp+LrXb8kVoaFhRJ1Iu+1oQkLSb/vqfNHRUQSHhGSrrTtZOfv1LFwwn2bNWyIihJctx82ly3Do4AHA8RwFKFGyJE2btWD37p0ey2XlfW7V7FbNfSPJdwXEmdOnAIiOOsGaVT/TrFXbdPOjo07w4qinGDV2EuFly6dOL1HyJoJDwzh6+CAAv27eSLlbKrot59HYOOredjMBBR3ju03vKssfR04RVqJw6jKd76vM74dOurTe4OKFAMcn+Wfvv5ePf9iea5kzc+b0aeLOnwcgMTGRTRvXU/6WChmWuxAXx69bN9OoabPUabdXrcbRI4c5/vcxkpIu89OPS2jU2LUel5yqWq06R44c4tixoyRdvszSxT/QOE02gCZNm7Fo4XcYY9i5YztFihQlODgkW201e86EhZVi08b1AJw6dZLDhw9Sukw4CfHxXLzo6IlIiI9nw/q1VKrk3vN70rLyPrdqdqvmzikr9kB4/+ykXPbiqKc5f+4svr6+PDliDEWLBbJo3mwAOnbrxZeffsT5c2d557XxgKP7/cPPHeOpw/47ioljnyUpKYlSpcsw8rmX3ZZz894TzI/8k/UfPEiyPYUdf8Xw6eKdfPhUa2pUDMEYOBx9jmHvLAOgVInCfPB0G7o+5/ha2xejOtCwRjg3BQbw11eP8vKXa/li6S56NbmNIZ3uAmDBmn1M/3G32x4DwMmTsYx7fhQpKSmkpKTQvFUbGjRqwrw5MwHo1rMPAKtW/Ezde+8jIKBQaltfX19GPDuGJx8bTEpKCh07d6VCpcpuzZt226PGvMBjEY+QkmKnS9fuVKpUmdmzvgGgV+++NGzUmDWrf6FD25b4+wcwbvzELNt6ipWzjx75NFu2bObs2TO0bdGYIUOHkZycDECPXn0YPOQxxj4/il7dOoKBJ4ePICgoiGPHjjJi+BOAY7ixTdsO1G/Q0GO5rbzPrZrdqrlzLG+PsGRKTCZjSHnBsTOX82awbKjc+x1vR8iREwue9naEHPP3y/xcF+U+yXZrPkV9bRZ8pVZe4+/rmbf2mwbMzJUn1MnP+3jsAM93QxhKKaWUcr98N4ShlFJKWU1e/5ZIZrSAUEoppbzMigWEDmEopZRSymXaA6GUUkp5m/U6ILSAUEoppbzNikMYWkAopZRSXmbFAkLPgVBKKaWUy7QHQimllPIyK/ZAaAGhlFJKeZkVCwgdwlBKKaWUy7QHQimllPI263VAaAGhlFJKeZsOYSillFLqhuC2AkJEbhOR5iJS5Krpbdy1TaWUUsqKRCRXbp7kliEMEXkSeBzYA3wqIv8xxixwzp4ILL1GuwggAmDCG+9x/0OD3BHP7fbN+o+3I+RItf8u8naEHNv7VmdvR7jh+Nqs1+UKkGw33o6QY7rPvcDXM/vcikMY7joHYjBwtzHmgoiUB+aKSHljzDtkcaqIMWYqMBXg0KlECx9xSimllAusVz+4rYCwGWMuABhjDolIExxFRDksuZuUUkoplZa7zoGIEpGaV+44i4kOwE1AdTdtUymllLIkPQfiHw8CyWknGGOSgQdFZIqbtqmUUkpZkp4D4WSMOZbFvLXu2KZSSimlPEd/SEoppZTyMu2BUEoppZTLtIBQSimllOusVz/oT1krpZRSynXaA6GUUkp5mQ5hKKWUUsplViwgdAhDKaWUUi7THgillFLKyyzYAaEFhFJKKeVtOoShlFJKqRuC5Xsg3pjwAhvXrqZ4UAmmfjUPgNUrlvHlpx9y9NBB/u+Tr7j19qrZbutK+9z07awZLF7wLcYY2nfuTvc+/dPN/3np98z8choAAYUKMXzk81SsXAWAC3HneX3iixw6sA9BGPHcOKpWr+m2rBVCivDhoDqp98veVIjXv99LqeL+tKgeRpI9hcOxF3n6y185n5CUrm2poADeeagWwcX8SUkxfL32EJ+uPADAc12rXrd9bnrphdFE/rKKEiVKMnv+ogzz4+LieH7U/4iKOoHdbqf/Qw/TqUt3ADq0aUahQoWx2WzYbDZmzPzWbTnzS+4r1kau5pXJE0ixp9C1e08GDY5IN98YwyuTJrBm9S/4B/jz8oTJ3H5H1Wy1dScr73fd59451l1hwQ4I6xcQrdp1plOPvrw2bkzqtPIVKvHCxLf4v1dfdrmtK+1zy8H9+1i84Fven/Y1fr5+PDv8Ue6p34gyZculLlPq5jK89eFnFC0WyMZ1kbw56SXen/Y1AO+99Qp16t3Hi5PeJCkpiUuJCW7NeyDmAq0nrQTAR2DLxDYs3XGciqFFmLTgd+wphtFd7uCJ1pWZ+N3v6dra7SmM+3Y3u4+eo3BBX5Y824TVe2LZFxXH6r0x122fmzp26kqvPv0YO+bZTOfPmfkVFSpW4u33PuLM6dN069SWtu074udXAIApn04nKCjIbfmuxaq5Aex2OxMnjGPKx58RGhrK/b170KRpMypWqpS6zJrI1Rw5fIhFS5axa+cOxo97ka9mzslWW3ey6n7Xfe6dY91VOoThBdXvupuixYqlm1a2fAXCy5XPUVtX2ueWI4cOcHvVGvj7B2Dz9aVGrdqs+WV5umWq1qhJ0WKBANxRrQaxsdEAXLx4gV2/bqVdp24A+Pn5UaRoxsfkLg1uC+bwyYv8fTqB1XtisacYALYdPEOp4gEZlo85f4ndR885sl9KZl9UHGHF/QGy1T431apdh8DAwGsvIMLFixcxxhAfH0+xwEBsNu/X3FbNDbB7107Cw8tRJjwcvwIFaNOuPatWpj/WV65YTsdOXRARatxZk7i488TGxmSrrTtZdb/rPrcGkdy5eZLlC4j8oHyFyuzcvpVz586SmJjAxnWRxEZHXXP5JYvmU7deAwBO/H2MwKAgXn35OYY82JPXJ4wlISHeU9HpdHcZFmzJePHV3vXLsfL36CzblilRiGrhgfx66EyO2rtb7779OHhwP62bN6J3906MeGY0Pj6Op4wgPD5kEP16d2Pe3FlezXm1vJw7JjqasFJhqfdDQkOJjk7/7xwTE01o2D/LhIaGERMdna223pRX97vu87z3HM0vrFmq5TPlbqlAn/4DGTksgoBCAVSsXAWbry3TZX/duoklC+fx9tTpgKN7ct8fexj29Chur1aD996czMzpn/LwkGFuz+1nE1rVCGPygvTDDMPa3IrdnsK8Tde8qjuFCtqYGlGXF+fu4kJissvtPWH92jVUqXI7Uz75gmNHjzA0YiB31apNkSJFmDb9a4JDQjl96hRDhwykfPkK1Kpd5/or9YC8nNtgMkzL0HVrMl8mW229KK/ud93nee85mhkfn7yzX7NLeyDyiHadujFl+mze/ugLihYLpHSZchmW2b/vD96YOJZxr/0fgYHFAQgOCSU4OJTbq9UAoFGzluz7Y49HMjetGsquo+c4GXcpdVqPe8JpUS2MJz7bes12vj7C1MF1mb/pKEu2n0g3LzvtPWXhgvk0a94SESG8bDluLl2GQwcdJ3wGh4QCUKJkSZo2a8Hu3Tu9GTWdvJw7NDSMqBP/9K7FREcTEhKSbpmQ0DCio/5ZJjo6iuCQkGy19aa8ut91n+e952hmdAhD5diZ06cAiI46wZpVP9OsVdt086OjTvDiqKcYNXYS4WXLp04vUfImgkPDOHr4IAC/bt5IuVsqeiRz59plWLD5n16CJneEMLRVZR7+aAOJSfZrtnu9/138FXWBj1fsTzc9u+09JSysFJs2rgfg1KmTHD58kNJlwkmIj+fixQsAJMTHs2H9WipVutWbUdPJy7mrVqvOkSOHOHbsKEmXL7N08Q80btos3TJNmjZj0cLvMMawc8d2ihQpSnBwSLbaelNe3e+6z/PeczS/EJNJ11VecOhUYraCTXrhGXb+uoVzZ88SVKIE/R95jKLFAvngzcmcO3uGwkWKUrFyFSa+/RGnYmN4a/JLjH/j/Wu2bdOxG2t/WZ5p++zy9XG9LvvPkIc4f+4svr6+PPaf/1GrTj0WzZsNQMduvXh9wlgiV/1EaNjNANhsNj783DGu99efe3lj4liSkpIoVboMI597OfWES1fUG7M428v6+9nYPKE19V9YRpxzCGLNiy0o4OfDmQuOr15uO3SaUd/sIDTQn9f61eTBDzZQp2IJ5v+3EXv+PkdKimNdryz8nRW/RV+zfXbsfauzC4/UYfTIp9myZTNnz56hZImSDBk6jORkx2Pp0asPsTHRjH1+FCdjY8HAgEGDadehE8eOHWXE8CcAxxBSm7YdGBTxqMvbz6m8ktvXlrOPO5Grf+HVyRNJSbHTpWt3Bg95jNmzvgGgV+++GGOYNH4ca9dG4u8fwLjxE6larfo127oq2Z6z17y8sN91n3t+nxcp6JnP9dWe+ylX3ox3j2/psX4IyxcQeVFOCoi8wJUCIq/JSQGh/p2cvpl5W07fzPIC3eee56kCovrzuVNA7HrZcwWEnkSplFJKeVleOjk1u6z5UVkppZRSXqU9EEoppZSXWbEHQgsIpZRSysssWD/oEIZSSimlXKc9EEoppZSX6RCGUkoppVxmwfpBCwillFLK26zYA6HnQCillFLKZdoDoZRSSnmZBTsgtIBQSimlvE2HMJRSSil1Q9AeCKWUUsrLLNgBoQWEUkop5W1WHMJwWwEhInUBY4zZLCJ3AG2AvcYY614zWimllFKAmwoIERkLtAV8ReQn4B5gFfCsiNxljJlwjXYRQATAW+9+yIBBg90Rz+0uJNq9HSFHtkxu7+0IOVZt5A/ejpAje9/o4O0IOXYy7rK3I+TITUULeDvCDcfXZr1P155mwQ4It/VA9ABqAgWBKKCMMea8iLwGbAQyLSCMMVOBqQBnE+zGTdmUUkqpPEWHMP6RbIyxA/Eist8Ycx7AGJMgIilu2qZSSillSRasH9z2Nc7LIlLI+ffdVyaKSCCgBYRSSillce4qIBoZY+IBjDFpCwY/4CE3bVMppZSyJBHJlVs2t1VcROaKyF4R2SMi94pICRH5SUT2Of8fdL31uKWAMMZcusb0k8aYXe7YplJKKWVVIrlzy6Z3gKXGmNuAO4E9wLPAcmNMZWC5836W9JcolVJKqRuEiBQDGgGfAhhjLhtjzgKdgS+ci30BdLneurSAUEoppbwst4YwRCRCRLakuUVctakKQCzwmYj8KiKfiEhhINQYcwLA+f+Q62XWX6JUSimlvCy3vsaZ9ucQrsEXqAUMM8ZsFJF3yMZwRWa0B0IppZTyMg+eA3EMOGaM2ei8PxdHQREtIqUcWaQUEHO9FWkBoZRSSt0gjDFRwFERqeKc1Bz4HVjIP9+SfAhYcL116RCGUkop5WUe/iXKYcBXIlIAOAA8jKNDYbaIDAKOAD2vtxItIJRSSikv82T9YIzZDtTOZFZzV9ajQxhKKaWUcpn2QCillFJephfTUkoppZTLLFg/5K8C4tKlSzw68EEuJ13GnpxMsxatiBg6LN0yX37+KT8u/h4Au93OoYMHWLpyDYGBxVm/NpI3X51ESoqdTl178NDAwR7L/u2sGSxe8C3GGNp37k73Pv3Tzf956ffM/HIaAAGFCjF85PNUrOw4ifZC3Hlen/gihw7sQxBGPDeOqtVreiT3kUMHeWn0iNT7x48fY2DEE/S8v3+GZff8touhA/sxduLrNGneCoDJ455j/ZrVBAWV4PNZ37k1a4WQwrz3UK3U++E3FeKtxX/y7eZjvDegFmVKFOLY6Xge/2wb5xOS0rUt6OvDrCfrU9DXB5uPsGTHCd5a8icA7z1UiwohhQEoFuDH+YQk2r0W6dbHsjZyNa9MnkCKPYWu3XsyaHD634oxxvDKpAmsWf0L/gH+vDxhMrffUTVbbd3Nqse6lfe5VbNbNXdO+FiwgshXBUSBAgV4/+NpFCpUmOSkJCIefoB7GzSieo07U5fpP2AQ/QcMAiDyl5V8M2M6gYHFsdvtvDZpPO9+9AkhoaEM6Nebho2bUqFiJbfnPrh/H4sXfMv7077Gz9ePZ4c/yj31G1GmbLnUZUrdXIa3PvyMosUC2bgukjcnvcT7074G4L23XqFOvft4cdKbJCUlcSkxwe2Zryhb/hY+/fpbwFGQ9WjXjIZNM56HY7fbmfLeW9Spd1+66W07dKFbr/uZOHa027MeiLmY+sbuI7BxXAt+3BnFYy0qse7Pk3z4834ea1GRoS0qMnnR3nRtLyWncP9764m/bMfXR5j7n/qs+j2GXw+f5YkvtqUuN6bL7cQlJLv1cdjtdiZOGMeUjz8jNDSU+3v3oEnTZlSs9M+xuiZyNUcOH2LRkmXs2rmD8eNe5KuZc7LV1p2seqxbeZ9bNbtVc99I8tVJlCJCoUKOT4LJyckkJydn2S20bMliWrVpB8Dvu3dRJrwspcuE4+dXgJat27J61QpPxObIoQPcXrUG/v4B2Hx9qVGrNmt+WZ5umao1alK0WCAAd1SrQWxsNAAXL15g169badepGwB+fn4UKVrMI7mvtm3zBm4uE05YqZszzJs362saN21JUFCJdNPvrFU79XF50n233sThk/H8fSaBltVCmbvpGABzNx2jZfWwTNvEX7YD4GsTfG0+mEyWaV/zZhZuO+6u2ADs3rWT8PBylAkPx69AAdq0a8+qlemPl5UrltOxUxdEhBp31iQu7jyxsTHZautOVj3WrbzPrZrdqrlzysMX08oV+aqAAEfV+kCvrrRp1oC69epTrfqdmS6XmJDAhnWRNG3REoCYmGhCw/554wgJDSM25ro/xJUryleozM7tWzl37iyJiQlsXBdJbHTUNZdfsmg+des1AODE38cIDAri1ZefY8iDPXl9wlgSEuI9kvtqy5ctoXnrdhmmx8ZEE7lqOZ269/JCqsx1rPXPG31w0YLEnndcQDb2/CVuKlog0zY+Aov/15CtE1qx5o9Yth8+m25+3YolOBl3iUOxF92aPSY6mrBSaY/VUKKjo9Mvc9XxHBoaRkx0dLbaupNVj3Ur73OrZrdq7pzy5OW8c0u+KyBsNhszZs9n0Y8r+W33Lvb/tS/T5SJXr6JGzVoEBhZ3TDAZP0966t+i3C0V6NN/ICOHRfDs8EepWLkKNl9bpsv+unUTSxbOY/ATTwGOgmnfH3vo1K03U6bPwT8ggJnTP/VM8DSSkpJYt3pV6rkNab375isMGfYUNlvmj8nT/GxCi2phLN7uWk9BioF2r0Vy79ifubNccW4tVTTd/E613N/7AGAy6fvI8MKR6fEs2WvrRlY91q28z62a3aq5byT56hyItIoWK8bdteuwfm0kFStVzjD/p6X/DF+Ao8chOuqfT0Ix0VHcFHzdi5HlmnaduqV2zX7y4TsEB4dmWGb/vj94Y+JYJr31YWrhExwSSnBwKLdXqwFAo2YtvVJAbFwXSeXbbqdEyZsyzPtjz2+MG/M/AM6dPcOGdZHYbDYaNnHpN0tyTZPbQ9h97Bwn4y4DEBt3ieBijl6I4GIFU6dfy/mEZDb8dYrGtwXz54k4AGw+Qus7S9HRzSdPguNTVtSJtMdqNCEh6Y/Vq4/n6OgogkNCSEpKum5bd7PisW7lfW7V7FbNnVM+Fqxv8lUPxJnTp4k7fx6AxMRENm1cT/lbKmRY7kJcHL9u3Uyjps1Sp91etRpHjxzm+N/HSEq6zE8/LqFR46YezH4KgOioE6xZ9TPNWrVNNz866gQvjnqKUWMnEV62fOr0EiVvIjg0jKOHDwLw6+aNlLulosdyX7H8x8U0b5Vx+AJg1oIfmbVwGbMWLqNxs1Y89cxzXiseADrdfTOLtv2dev/n3dH0qFsGgB51y/DT7oxdnSUKF6BYgKPeLujnw3233sT+mAup8xvcehMHoi8QdS7RzemharXqHDlyiGPHjpJ0+TJLF/9A4zTHMkCTps1YtPA7jDHs3LGdIkWKEhwckq227mbFY93K+9yq2a2aO6esOISRr3ogTp6MZdzzo0hJSSElJYXmrdrQoFET5s2ZCUC3nn0AWLXiZ+reex8BAYVS2/r6+jLi2TE8+dhgUlJS6Ni5KxUy6blwlxdHPc35c2fx9fXlyRFjKFoskEXzZgPQsVsvvvz0I86fO8s7r40HHEM1H34+C4Bh/x3FxLHPkpSURKnSZRj53Mseyw2QmJjAlk3r+e/osanTFnzryNa5e+8s27405n9s37qZc2fP0qN9cx6OGEr7zt3dltXfz4cGVYIZPWtX6rQPf/6L9x++m171ynL8TAJDP9sKQEixgrzS904enrKJkMCCvNGvJj4+go/AD7+eYMVv/5wj4zin4u8M23MHX19fRo15gcciHiElxU6Xrt2pVKkys2d9A0Cv3n1p2Kgxa1b/Qoe2LfH3D2Dc+IlZtvUkKx7rVt7nVs1u1dw3EjGZjCHlBWcT7HkzWDZcSLR7O0KO+Nos2IfmVO/5H70dIUf2vtHB2xFy7HpDPXnVtU6SVSoz/r545IWx/ZRNufKe98OQuh57Ic9XPRBKKaWUFYln6pRcpQWEUkop5WV6EqVSSimlbgjaA6GUUkp5mRV/p0ILCKWUUsrLLFg/6BCGUkoppVynPRBKKaWUl+nlvJVSSinlMgvWD1pAKKWUUt5mxZMo9RwIpZRSSrlMeyCUUkopL7NgB4QWEEoppZS35auTKEWkVlYNjTHbcj+OUkoppawgqx6IN7KYZ4C8fXF1pZRSyiKs1/+QRQFhjGmamxsSkenGmAdzc51KKaVUfmDFb2Fc9xwIESkEPA2UNcZEiEhloIox5vss2iy8ehLQVESKAxhjOl2jXQQQAfDeB1MYNDgiWw8ir/H10S+3eNr2yW29HSFHQh/80tsRcmzb2z28HeGGk2w33o6QI7426705quvLzkmUnwFbgfrO+8eAOcA1CwigDPA78AmO4Q4BapP1sAjGmKnAVIDEZKz5TFFKKaVclF8v513RGPMqkARgjEng+sM1tXEUHWOAc8aYVUCCMeYXY8wv/yKvUkople+ISK7cPCk7PRCXRSQAR08CIlIRuJRVA2NMCvCWiMxx/j86m9tSSimlbjgWPAUiW2/qY4GlQLiIfAXcBwzIzsqNMceAniLSHjif05BKKaWUyluuW0AYY34SkW1APRxDF/8xxpx0ZSPGmB+AH3IWUSmllMrf8uW3MJwaAw1wDGP4AfPdlkgppZS6weTLkyhF5APgUWAXsBsYIiLvuzuYUkoppfKu7PRANAaqGWOunET5BY5iQimllFK5wIpDGNn5GucfQNk098OBne6Jo5RSSt14JJdunpTVxbQW4TjnIRDYIyKbnPfvAdZ5Jp5SSimV/+Wrq3ECr3sshVJKKaUsJauLaekvRiqllFIeYMEOiGx9C6OeiGwWkQsicllE7CKiPwqllFJK5RIr/pR1dk6ifA/oC+wDAoBHnNOUUkopdYPK1g9JGWP+EhGbMcYOfCYiehKlUkoplUusOISRnQIiXkQKANtF5FXgBFDYvbFybm3kal6ZPIEUewpdu/dk0OCIdPONMbwyaQJrVv+Cf4A/L0+YzO13VM1WW3d66YXRRP6yihIlSjJ7/qIM8+Pi4nh+1P+IijqB3W6n/0MP06lLdwA6tGlGoUKFsdls2Gw2Zsz8VnNnw6VLl3h04INcTrqMPTmZZi1aETF0WLplvvz8U35c7Lhyvd1u59DBAyxduYbAwOKsXxvJm69OIiXFTqeuPXho4GC3Za1UqhifDWuYer98SBEmzt1B3crBVCpVDIDAwgU4d/EyDUf/kK22Hy7dS/VyQbw18B4K+tmwpxie/mwj2/afytXsb00ay6Z1qykeVIIPpzv+jT99/002rluNr68fpUqX4alRL1GkaLEMbb+b8xU/LpqHMYY2HbvRpdcDqfMWzv2GRfNmYrPZqHNvQwYNfSpXc19NX1s8/xy16j7Pifz2LYwr+uMY6ngCeArH70B0c2eonLLb7UycMI4pH39GaGgo9/fuQZOmzahYqVLqMmsiV3Pk8CEWLVnGrp07GD/uRb6aOSdbbd2pY6eu9OrTj7Fjns10/pyZX1GhYiXefu8jzpw+TbdObWnbviN+fgUAmPLpdIKCgjySNS2r5gYoUKAA7388jUKFCpOclETEww9wb4NGVK9xZ+oy/QcMov+AQQBE/rKSb2ZMJzCwOHa7ndcmjefdjz4hJDSUAf1607BxUypUdM/x8teJ86mFgY8Ie9/vzvdbjvLh0r2py4zvdzfn4y9nuy3AuL61mDxvJz/vOE7Lmjczrm8tOoz/KVezt2jbiY7d+vDGhOdSp91Vpx4DhjyJzdeXaR++zewZ0xj42PB07Q4d+IsfF83jrakz8PP14/kRj1Pn3oaUDi/Hjm2b2bBmFR98Pge/AgU4e+Z0rma+mr62eP45auV9nhMWrB+ufw6EMeawMSbRGHPeGPOSMeZpYKIHsrls966dhIeXo0x4OH4FCtCmXXtWrVyebpmVK5bTsVMXRIQad9YkLu48sbEx2WrrTrVq1yEwMPDaC4hw8eJFjDHEx8dTLDAQm837V0i3am5wnLRUqJCjMy05OZnk5OQsn8TLliymVZt2APy+exdlwstSukw4fn4FaNm6LatXrfBEbJpUC+NgdBxHT15MN71rvXLMXX/IpbYGKBbgB0CxgAJEnUnI9bzVa95N0WLpexdq1a2PzddxHNxWtQYnY6MztDt6+ABV7qiBv38ANl9fqtW8m3WrHfv4h+9m0/OBh/Er4HiTKx5UItdzp6WvLZ5n5X1+o8jOSZSZuTdXU+SSmOhowkqFpd4PCQ0lOjr9C1NMTDShYf8sExoaRkx0dLbaelPvvv04eHA/rZs3onf3Tox4ZjQ+Po5/PkF4fMgg+vXuxry5s7ycNL28nttut/NAr660adaAuvXqU636nZkul5iQwIZ1kTRt0RLIeByFhIYRGxPjkczd7i2foVCof1sIsecSORAV51LbZ6dvZtz9d/Pbu90Y3+9uXpr1qxsSZ23ZD99R+54GGaaXu6USu3ds5fy5syQmJrBlwxpOxjiek8ePHua3HdsYHvEAI58YxJ97drs1o762eP45mp/3eWas+C0M75eZuchgMkzLsENN5stkq60XrV+7hipVbmfKJ19w7OgRhkYM5K5atSlSpAjTpn9NcEgop0+dYuiQgZQvX4Fatet4OzKQ93PbbDZmzJ5P3PnzjHz6Sfb/tY+KlSpnWC5y9Spq1KxFYGBxx4RMjyM3hwX8bD60u7sML81M/0bfo3555q476HLbQS2qMPrLLSzcfISu95TjvYh76TzxZ7dkz8zM6R9js9lo2qpdhnlly1egZ7+HGfPUo/gXKsQtlW7FZrMBjsLvQlwcb035kj/37GbS2JFMm/WD256z+tri+edoft7nmcnpp3lvumZmEal1jdvdOC7pneeEhoYRdSIq9X5MdDQhISHplgkJDSM66p9loqOjCA4JyVZbb1q4YD7NmrdERAgvW46bS5fh0MEDAASHhAJQomRJmjZrwe7deedSJVbJXbRYMe6uXYf1ayMznf/T0n+GLyDjcRQTHcVNwe4/XlrWvJkdB08Tez4xdZrNR+hYpyzzNhx2uW3fRhVYuPkIAPM3HqZWhZLuCZ6Jn5csZNO6SP73wsRrvri37tCVd6fN5LX3plG0aDFuDndcluem4FDqN26GiFDljuqI+HD+7Bm3ZdXXFs8/R/PzPs8vsip63rjG7XVgbxbtvKZqteocOXKIY8eOknT5MksX/0Djps3SLdOkaTMWLfwOYww7d2ynSJGiBAeHZKutN4WFlWLTxvUAnDp1ksOHD1K6TDgJ8fFcvHgBgIT4eDasX0ulSrd6M2o6eTn3mdOniTvv+E20xMRENm1cT/lbKmRY7kJcHL9u3UyjNMfD7VWrcfTIYY7/fYykpMv89OMSGjVu6vbMPerfkmH4okm1Uvx5/DzHT8e73DbqTAINbne8STSuGsaB6KyHQHLLlo1rmfPV54yd9Db+/gHXXO7KyZEx0SdYt3oFjVu0BaBew6bs2LoZgGNHDpOcnESx4u470U9fWzz/HM3P+zwz+WoIwxjj/lfDXObr68uoMS/wWMQjpKTY6dK1O5UqVWb2rG8A6NW7Lw0bNWbN6l/o0LYl/v4BjBs/Mcu2njJ65NNs2bKZs2fP0LZFY4YMHUZycjIAPXr1YfCQxxj7/Ch6desIBp4cPoKgoCCOHTvKiOFPAI5u3TZtO1C/QcOsNqW5nU6ejGXc86NISUkhJSWF5q3a0KBRE+bNmQlAt559AFi14mfq3nsfAQGFUtv6+voy4tkxPPnYYFJSUujYuSsV3Hy8BBSw0bRaKYZ/siHd9O73lufbq4YvwooH8G7EvfR8dUWWbZ/8ZD2vPFgHm49wKSmF/1w1Pze88uKz7Px1C+fPnaV/t1Y8MPAxZs+YRlLSZcY8/SgAVarWYNiI5zh1MoZ3XnmJca+9D8CE5/7L+XPn8PX1ZehToyjq/Kpnq/ZdeHvSWB57sDu+vn48Pfplt7546muL55+jVt7nOeGTt0dYMiUmkzGkvCAxOZNBLItItls2umUlp6R4O0KOlBv0tbcj5Ni2t3t4O0KOlC5x7R6PvM6qry2+Ngu+Ozr5+3rmKtnDF+zNlX/ctzvf5rGdna9OolRKKaWsyIo9EFpAKKWUUl6W178lkpnsXI1TROQBEXnBeb+siNR1fzSllFLqxuAjuXPzaOZsLPMBjh+O6uu8Hwe877ZESimllMrzsjOEcY8xppaI/ApgjDnjvLiWUkoppXKBBUcwslVAJImIDcfP5iMiwYA1T3lXSiml8iArXo0zO0MY/wfMB0JEZAKwhjx6MS2llFJKecZ1eyCMMV+JyFagOSBAF2PMHrcnU0oppW4QVrwWxnULCBEpC8QDi9JOM8YccWcwpZRS6kZhwRGMbJ0D8QOO8x8E8AduAf4Aqroxl1JKKXXDsOI5ENkZwqie9r6I1AKGuC2RUkoppfI8l3+J0hizTUQ8c0F4pZRS6gZgwQ6IbJ0D8XSauz5ALSDWbYmUUkqpG0x+vRZG0TR/J+M4J+Jb98RRSimllBVkWUA4f0CqiDHmf/9mIyLSAKgL7DbGLPs361JKKaXym3x1EqWI+Bpjkp0nTbpERDYZY+o6/x4MPI7jx6jGikgtY8zka7SLACIA3nr3QwYMGuzqpvOEZHuuXNbd4/z9bN6OcMM5/On93o6QY+3eXevtCDmycGh9b0fIMas+R636mgiAr2fe2C1YP2TZA7EJx/kO20VkITAHuHhlpjFmXhZt/dL8HQG0NMbEisjrwAYg0wLCGDMVmApwNsHKR5xSSimVv2XnHIgSwCmgGf/8HoQBsiogfEQkCMdJl2KMiQUwxlwUkeR/F1kppZTKXzx9EqXzFIUtwN/GmA4iUgKYBZQHDgG9jDFnslpHVgVEiPMbGLv5p3C44nq9A4HAVmcbIyJhxpgoESly1XqUUkqpG554/q3xP8AeoJjz/rPAcmPMZBF51nn/maxWkFUBYQOu9YafZQFhjCl/jVkpQNes2iqllFI3Gk/2QIhIGaA9MAG48lMNnYEmzr+/AFbxLwqIE8aYcf8q5VWMMfHAwdxcp1JKKaUc0n4ZwWmq8/zCtN4GRpL+ZxpCjTEnAIwxJ0Qk5HrbyqqA0KEGpZRSygNyqwci7ZcRMiMiHYAYY8xWEWnyb7aVVQHR/N+sWCmllFLZI577Hud9QCcRaYfjApnFRGQGEC0ipZy9D6WAmOut6JqXIDfGnM61uEoppZTyOmPMKGNMGee5in2AFcaYB4CFwEPOxR4CFlxvXS5fTEsppZRSuSsPXAtjMjBbRAYBR4Ce12ugBYRSSinlZd74JUpjzCoc37bAGHMKF09d0AJCKaWU8jIrXgvjmudAKKWUUkpdi/ZAKKWUUl6WB86BcJkWEEoppZSXWXAEQ4cwlFJKKeW6fNUDcenSJR4d+CCXky5jT06mWYtWRAwdlm6ZLz//lB8Xfw+A3W7n0MEDLF25hsDA4qxfG8mbr04iJcVOp649eGjgYI/m796hJYUKFcbH5oPN5su0GbPTzd+2ZRPPPj2MUqVLA9C4aQsGRgwFIC7uPJNffoEDf/2FiDB67MtUq1HT7ZlfemE0kb+sokSJksyevyjD/Li4OJ4f9T+iok5gt9vp/9DDdOrSHYAObZpRqFBhbDYbNpuNGTO/dXvetKx6vFgtd5GCNka1uZUKNxXGABOX/EFw0YIMuq8c5UsW4pEvf2Vv1IVM245ucyv3VSzBmfgkHvhsa+r0plVuylb73GTF5ydY+zlq5eyu8rHgjz/nqwKiQIECvP/xNAoVKkxyUhIRDz/AvQ0aUb3GnanL9B8wiP4DBgEQ+ctKvpkxncDA4tjtdl6bNJ53P/qEkNBQBvTrTcPGTalQsZJHH8O7Uz6jeFDQNeffedfdvPbOBxmmv/3aJO65twETXn2bpKTLJCYmujNmqo6dutKrTz/Gjnk20/lzZn5FhYqVePu9jzhz+jTdOrWlbfuO+PkVAGDKp9MJyuLxupNVjxer5R7evBIbDp5hzII9+PoI/n4+xCUmM/q73xnZqnKWbRfvjmbur8d5oV2VdNMPxF7MVvvcZrXnJ1j7OWrl7K7SIQwvExEKFSoMQHJyMsnJyVn+oyxbsphWbdoB8PvuXZQJL0vpMuH4+RWgZeu2rF61whOx/7WLFy6w49etdHRW3n5+BShatNh1WuWOWrXrEBgYeO0FRLh48SLGGOLj4ykWGIjNljfqVqseL1bKXaiAjZplAlm0M8qRN8Vw4ZKdw6cTOHI64brttx87x/mEpAzTs9s+L/Dm8xOs/Ry1cnZX+Uju3Dya2bObcz+73c4DvbrSplkD6tarT7Xqd2a6XGJCAhvWRdK0RUsAYmKiCQ0LS50fEhpGbMx1fwo8V4kITz0+mIH9erJg3uxMl9m9azsP9enKf4cN4cD+vwD4+++jFA8KYsKLYxhwf3cmjXuBhIR4T0a/pt59+3Hw4H5aN29E7+6dGPHMaHx8HIedIDw+ZBD9endj3txZXsln1ePFKrlLF/fnbMJlxrS9lc8fqsWzbSrj72fNl538+PyEvP8czYqVs+cH1nwmZ8FmszFj9nwW/biS33bvYv9f+zJdLnL1KmrUrEVgYHHHBGMyLOPpLqUPp83gs6/n8sa7HzFv9jds37Yl3fwqt93Bt9//xBcz59O9dz9G/dcx7m232/lz7x669ujD519/S0BAAF9+9olnw1/D+rVrqFLldn5cvppv5szn1Ykvc+GCY7x62vSv+Xr2PN794GNmz/yabVs2ezyfVY8Xq+S2+Qi3hhZl/vYTDPhiG4mXU+h/T7j7NuhG+fH5CXn/OZoVK2e/mo9Irtw8mtmjW/OgosWKcXftOqxfG5np/J+W/tOtC45PYtFRUan3Y6KjuCn4updDz1XBzu0FlShJo6Yt+H33rnTzCxcpktp1Xb9BI5KTkzl75gwhIaEEh4RStXoNAJq0aMWfe/d4NPu1LFwwn2bNWyIihJctx82ly3Do4AEAgkNCAShRsiRNm7Vg9+6dXstpxeMF8n7umLhLxMZd4vcTcQCs/DOWKqFF3LY9d8qPz0+wznM0M1bOfjWR3Ll5Ur4qIM6cPk3c+fMAJCYmsmnjesrfUiHDchfi4vh162YaNW2WOu32qtU4euQwx/8+RlLSZX76cQmNGjf1WPaEhHguXryY+vemDeuoUCn9iW2nTsZinJ8gf9+9E5OSQmDx4pS8KZiQ0DAOHzoIwNZNGyhfoaLHsmclLKwUmzauB+DUqZMcPnyQ0mXCSYiP5+JFxyeFhPh4NqxfS6VKt3o0m1WPFyvlPn0xiejzlyhbIgCA2uWCOHgq73TfZ1d+fX5C3n6OXo+Vs+cH1jzb5BpOnoxl3POjSElJISUlheat2tCgURPmzZkJQLeefQBYteJn6t57HwEBhVLb+vr6MuLZMTz52GBSUlLo2LkrFSp57gzv06dOMXrEkwAk2+20atOeevUbMt85dte1R29WLl/G/Lmz8LXZKFDQn5cmvZ56DfmnRo7mpeeeITkpiZtLl2H0i+M9knv0yKfZsmUzZ8+eoW2LxgwZOozk5GQAevTqw+AhjzH2+VH06tYRDDw5fARBQUEcO3aUEcOfABxdvG3adqB+g4YeyXyFVY8Xq+V+a/lfjO1wG34+wvFziUxY/CeNKpfk6RaVKB7gx+vdq7Ev5gJPzdnNTUUK8GzrWxnx7W4AXup4G3eFB1I8wI/vHruHT9Yc5vtdUdds7y5WfX6CtZ+jVs7uKiteC0NMJmOiecHZBHveDJYNyRaN7u9n83aEHEtOSfF2hBtOu3fXejtCjiwcWt/bEXLMys9RqypS0DPv7NM2H8mVN46Bdcp6rBLJV0MYSimllPKMfDWEoZRSSlmRFT/NawGhlFJKeZlY8BwILSCUUkopL7Ne+WDNXhOllFJKeZn2QCillFJeZsWvcWoBoZRSSnmZ9coHHcJQSimlVA5oD4RSSinlZRYcwdACQimllPI2/RqnUkoppVxmxfMJrJhZKaWUUl6mPRBKKaWUl+kQhlJKKaVcZr3yQYcwlFJKKZUDbumBEJF7gD3GmPMiEgA8C9QCfgcmGmPOuWO7SimllBXpEMY/pgF3Ov9+B4gHXgGaA58B3TJrJCIRQATAex9MYdDgCDfFc6+zSUnejnDD8fezeTvCDWfZfxp6O0KOlH54hrcj5NgfH/XxdoQc8fezcme3Z15brLiH3FVA+Bhjkp1/1zbG1HL+vUZEtl+rkTFmKjAVIDEZ46ZsSimlVJ5ixR4IdxU9u0XkYeffO0SkNoCI3Arox3OllFLK4txVQDwCNBaR/cAdwHoROQB87JynlFJKKSfJpZsnuWUIw3mS5AARKQpUcG7nmDEm2h3bU0oppazMgiMY7v0dCGNMHLDDndtQSimllOfpD0kppZRSXuZjwZ+S0gJCKaWU8jIrDmFY8aunSimllPIy7YFQSimlvEx0CEMppZRSrrLiEIYWEEoppZSXWfEkSj0HQimllFIu0x4IpZRSyst0CEMppZRSLrNiAaFDGEoppZRyWb7rgVgbuZpXJk8gxZ5C1+49GTQ4It18YwyvTJrAmtW/4B/gz8sTJnP7HVWz1dadjhw6yEujR6TeP378GAMjnqDn/f0zLLvnt10MHdiPsRNfp0nzVgBMHvcc69esJiioBJ/P+s5TsXnphdFE/rKKEiVKMnv+ogzz4+LieH7U/4iKOoHdbqf/Qw/TqUt3ADq0aUahQoWx2WzYbDZmzPzWY7nBuseKlbNb6XipVKoYnw1rmHq/fEgRJs7dQd3KwVQqVQyAwMIFOHfxMg1H/5Ctth8u3Uv1ckG8NfAeCvrZsKcYnv5sI9v2n3Lb47Dqa8ulS5d4dOCDXE66jD05mWYtWhExdFi6Zb78/FN+XPw9AHa7nUMHD7B05RoCA4uzfm0kb746iZQUO5269uChgYM9lj0n9GucXma325k4YRxTPv6M0NBQ7u/dgyZNm1GxUqXUZdZErubI4UMsWrKMXTt3MH7ci3w1c0622rpT2fK38OnX36Y+jh7tmtGwafNMH+OU996iTr370k1v26EL3Xrdz8Sxoz2S94qOnbrSq08/xo55NtP5c2Z+RYWKlXj7vY84c/o03Tq1pW37jvj5FQBgyqfTCQoK8mRkwNrHipWzW+l4+evE+dTCwEeEve935/stR/lw6d7UZcb3u5vz8Zez3RZgXN9aTJ63k593HKdlzZsZ17cWHcb/5LbHYdXXlgIFCvD+x9MoVKgwyUlJRDz8APc2aET1GnemLtN/wCD6DxgEQOQvK/lmxnQCA4tjt9t5bdJ43v3oE0JCQxnQrzcNGzelQkXPHOc54WO9+iF/DWHs3rWT8PBylAkPx69AAdq0a8+qlcvTLbNyxXI6duqCiFDjzprExZ0nNjYmW209ZdvmDdxcJpywUjdnmDdv1tc0btqSoKAS6abfWas2RYsFeipiqlq16xAYmMV2Rbh48SLGGOLj4ykWGIjN5v261crHipWzW/V4aVItjIPRcRw9eTHd9K71yjF3/SGX2hqgWIAfAMUCChB1JsEdkTNlpdcWEaFQocIAJCcnk5ycnOV5AsuWLKZVm3YA/L57F2XCy1K6TDh+fgVo2botq1et8ETsHJNc+s+T8lUBERMdTVipsNT7IaGhREenv4J4TEw0oWH/LBMaGkZMdHS22nrK8mVLaN66XYbpsTHRRK5aTqfuvbyQKmd69+3HwYP7ad28Eb27d2LEM6Px8XEcdoLw+JBB9OvdjXlzZ3k0l5WPFStnv568erx0u7d8hkKh/m0hxJ5L5EBUnEttn52+mXH3381v73ZjfL+7eWnWr25InDmrvbbY7XYe6NWVNs0aULdefapVvzPT5RITEtiwLpKmLVoCGY//kNAwYmNiPJL5RpKvCgiDyTBNri5ZTebLZKutByQlJbFu9arU8ce03n3zFYYMewqbzebxXDm1fu0aqlS5nR+Xr+abOfN5deLLXLhwAYBp07/m69nzePeDj5k982u2bdnssVxWPlasnP168uLx4mfzod3dZfhuw+F003vUL8/cdQddbjuoRRVGf7mFqsPmMfrLLbwXca9bcl/Niq8tNpuNGbPns+jHlfy2exf7/9qX6XKRq1dRo2YtAgOLOyZkevy7MWguEMmdmyd5v28wF4WGhhF1Iir1fkx0NCEhIemWCQkNIzrqn2Wio6MIDgkhKSnpum09YeO6SCrfdjslSt6UYd4fe35j3Jj/AXDu7Bk2rIvEZrPRsEnG8cy8YuGC+Tw8cDAiQnjZctxcugyHDh6gWvUaBIeEAlCiZEmaNmvB7t07qVW7jkdyWflYsXL268mLx0vLmjez4+BpYs8npk6z+Qgd65Sl8ZjFLrft26gCz0x3FD/zNx7m/wbXc0/wq1j5taVosWLcXbsO69dGUrFS5Qzzf1r6z/AFZDz+Y6KjuCk47xznmbHiSZT5qgeiarXqHDlyiGPHjpJ0+TJLF/9A46bN0i3TpGkzFi38DmMMO3dsp0iRogQHh2SrrScs/3ExzVtl7GIEmLXgR2YtXMashcto3KwVTz3zXJ55gl9LWFgpNm1cD8CpUyc5fPggpcuEkxAfz8WLjk+WCfHxbFi/lkqVbvVYLisfK1bOfj158XjpUf+WDMMXTaqV4s/j5zl+Ot7ltlFnEmhwu6MYalw1jAPRWQ+B5BarvbacOX2auPPnAUhMTGTTxvWUv6VChuUuxMXx69bNNEpzHN9etRpHjxzm+N/HSEq6zE8/LqFR46Yey36jyFc9EL6+vowa8wKPRTxCSoqdLl27U6lSZWbP+gaAXr370rBRY9as/oUObVvi7x/AuPETs2zrSYmJCWzZtJ7/jh6bOm3Bt46x3s7de2fZ9qUx/2P71s2cO3uWHu2b83DEUNp37u7WvACjRz7Nli2bOXv2DG1bNGbI0GEkJycD0KNXHwYPeYyxz4+iV7eOYODJ4SMICgri2LGjjBj+BOAY52zTtgP1GzTMalO5ysrHipWzW+14CShgo2m1Ugz/ZEO66d3vLc+3Vw1fhBUP4N2Ie+n56oos2z75yXpeebAONh/hUlIK/7lqvjtY8bXl5MlYxj0/ipSUFFJSUmjeqg0NGjVh3pyZAHTr2QeAVSt+pu699xEQUCi1ra+vLyOeHcOTjw0mJSWFjp27UsHDr+eusuK3MMRkMlaUFyQmZzJYaxFn45O8HSFHihS0bj3pa7Pgs8/iku3WfIqWfniGtyPk2B8f9fF2hBzx97NuZ3fxAM+8uET+eSZXnlANbw3y2Iuhdf9VlVJKKeU11v3IqZRSSuUTef1bIpnRAkIppZTyMgvWD1pAKKWUUt7mY8EuCD0HQimllFIu0x4IpZRSysus1/+gBYRSSinlfRasIHQIQymllFIu0x4IpZRSysuseC0MLSCUUkopL7PglzC0gFBKKaW8zYL1g54DoZRSSinXaQ+EUkop5W0W7ILQAkIppZTyMiueROmWIQwReVJEwt2xbqWUUkrljIiEi8hKEdkjIr+JyH+c00uIyE8iss/5/6DrrsuYXLkE+dUBzwEXgf3AN8AcY0xsNtpFABEA730w5e5BgyNyPZtS6sZ2Nj7J2xFy7JbGT3k7Qo6c2fyetyPkmL+vZ7oGth46nytvxneXL5ZlXhEpBZQyxmwTkaLAVqALMAA4bYyZLCLPAkHGmGeyWpe7hjAOAHcDLYDewEsishVHMTHPGBOXWSNjzFRgKkBiMrlf2SillFJ5kKcGMIwxJ4ATzr/jRGQPUBroDDRxLvYFsArIsoBw17cwjDEmxRizzBgzCLgZ+ABog6O4UEoppdQVkjs3EYkQkS1pbtfsyheR8sBdwEYg1FlcXCkyQq4X2V09EOmKKWNMErAQWCgiAW7aplJKKXVDS9uTnxURKQJ8Cww3xpyXHPySlbsKiN7XmmGMSXDTNpVSSilL8uS3METED0fx8JUxZp5zcrSIlDLGnHCeJxFzvfW4ZQjDGPOnO9arlFJK5UciuXO7/nZEgE+BPcaYN9PMWgg85Pz7IWDB9dalvwOhlFJK3TjuA/oDu0Rku3PaaGAyMFtEBgFHgJ7XW5EWEEoppZSXefBbGGuy2FxzV9alBYRSSinlbdb7IUq9mJZSSimlXKc9EEoppZSXWfFaGFpAKKWUUl6Wg59h8DotIJRSSikvs2D9oOdAKKWUUsp12gOhlFJKeZsFuyC0gFBKKaW8zIonUea7IYy1kavp1L41Hdq05NOPM15PxBjD5Inj6dCmJT26dmTP779lu607WTV3drafV7NbNXd2tq/Zc9eRQwcZdH/31FvbJvcw5+svM112z2+7aHpPDVYtX5Y6bfK45+jcqhEDenfxSN7H+zZhy5zRbJ07hifubwLAxOFd2D7vOTbNGsWsNwYTWCTz6xru/eElNs8ezYaZz7Lmq5Gp06vfWppVX/yXzbNHM/ftIRQt7O/2x2HFY+VGkq8KCLvdzsQJ4/jgo0+Yv/AHli7+nv1//ZVumTWRqzly+BCLlizjhRdfZvy4F7PdVnPnn+xWza3ZvZO9bPlb+PTrb/n062+Z+uVs/Av607Bpxh/ts9vtTHnvLerUuy/d9LYduvDa/33kkax3VCzFw93q07D/a9TtPYm2japRsWwwyzfs5e6eE6nbexL7Dsfwv4GtrrmONhHvUK/PZBr0ezV12ocv3M9z/7eAOr0msnDlDp56yKUfLXSZVY+VnPLUtTByU74qIHbv2kl4eDnKhIfjV6AAbdq1Z9XK5emWWbliOR07dUFEqHFnTeLizhMbG5Ottpo7/2S3am7N7r3sV2zbvIGby4QTVurmDPPmzfqaxk1bEhRUIt30O2vVpmixQI/ku+2WMDbtOkRCYhJ2ewqRW/+ic9M7Wb5hL3Z7CgCbdh2kdGhxl9ZbuVwIa7Y63oRXbNhLl+Y1czl5evnhWHGF5NLNk/JVARETHU1YqbDU+yGhoURHR6dfJiaa0LB/lgkNDSMmOjpbbd3FqrnButmtmhs0e1ZtPWH5siU0b90uw/TYmGgiVy2nU/deHs+U1m/7j9OgViVKBBYmwN+PNg2qUiYsKN0yD3a+lx/X/p5pe2MMiz54grVfjWRgt396Un7ff4IOTaoD0K1lLcqEBmXaPrfkh2PFJRasIPLVSZQGk2GaXN2nYzJfJltt3cSqucG62a2aGzR7lm3dLCkpiXWrVxHx+PAM89598xWGDHsKm83m0UxX++NgNG98/hPff/gEFxMusfPPv0lOtqfOHzmoNXZ7CjMXb860fbOH3+JE7DmCg4rw/UdP8MehKNZu28+QF7/ijZE9GDW4LT/8sovLSfZM2+cWqx8rN4J8VUCEhoYRdSIq9X5MdDQhISHplgkJDSM66p9loqOjCA4JISkp6bpt3cWqucG62a2aGzR7Vm3dbeO6SCrfdjslSt6UYd4fe35j3Jj/AXDu7Bk2rIvEZrPRsIl7zxXIzBffreeL79YD8NITHfk7+iwA/TreQ7tG1Wg75P+u2fZE7DkAYs9cYOGKndSpWp612/bz56FoOg59H4BKZUNo27CqWx+D1Y8VV+m3MLysarXqHDlyiGPHjpJ0+TJLF/9A46bN0i3TpGkzFi38DmMMO3dsp0iRogQHh2SrrebOP9mtmluzey87wPIfF9O8VcbhC4BZC35k1sJlzFq4jMbNWvHUM895pXgACA4qAkB4WBCdm93J7KVbaFn/dv47oAU9hk8hITEp03aF/AtQpFDB1L9b3Hsbv+0/nm6dIsKzg1vz8dw1bn0MVj9WXGXFkyjzVQ+Er68vo8a8wGMRj5CSYqdL1+5UqlSZ2bO+AaBX7740bNSYNat/oUPblvj7BzBu/MQs22ru/Jndqrk1u/eyJyYmsGXTev47emzqtAXfzgKgc/feWbZ9acz/2L51M+fOnqVH++Y8HDGU9p27uy3rN68/QonihUlKtjN88mzOxiXw1jO9KFjAl+8/fAKATbsO8eSEmZQKDuSDF+6n67APCSlZlFlvDgbA12Zj1pIt/LRuDwC92tRmSO9GACxYsZ3pCza4LT9Y+1i5UYjJZAwpL0hMzmQQSyml/qWz8Zl/+raCWxo/5e0IOXJm83vejpBj/r6eGVv4Myo+V97zbg0r5LF+iHzVA6GUUkpZkvVOgchf50AopZRSyjO0B0IppZTyMit+C0MLCKWUUsrLrPgzFVpAKKWUUl5mwfpBz4FQSimllOu0B0IppZTyNgt2QWgBoZRSSnmZFU+i1CEMpZRSSrlMeyCUUkopL9NvYSillFLKZRasH7SAUEoppbzOghWEngOhlFJKKZdpD4RSSinlZVb8FoZbCggRKQD0AY4bY34WkfuB+sAeYKoxxrrX01VKKaVymRVPohRjcuUS5OlXKvIVjuKkEHAWKALMA5o7t/nQNdpFABEA730w5e5BgyNyPZtSeUmyPfeff57ia7PgKx6QmGT3doQc8/Wx5qhzcL1h3o6QYwm/vueRA/3I6Uu58mJQtkRBjz0x3TWEUd0YU0NEfIG/gZuNMXYRmQHsuFYjY8xUYCpAYjLWfWVVSimlXGDFctxdBYSPcxijMI5eiEDgNFAQ8HPTNpVSSilLsuIQhrsKiE+BvYANGAPMEZEDQD1gppu2qZRSSlmU9SoItxQQxpi3RGSW8+/jIjIdaAF8bIzZ5I5tKqWUUspz3PY1TmPM8TR/nwXmumtbSimllJXpEIZSSimlXGbB+kF/iVIppZRSrtMeCKWUUsrLdAhDKaWUUi6z4k9Z6xCGUkoppVymPRBKKaWUt1mvA0ILCKWUUsrbLFg/aAGhlFJKeZsVT6LUcyCUUkop5TLtgVBKKaW8TL+FkQesjVxNp/at6dCmJZ9+PDXDfGMMkyeOp0OblvTo2pE9v/+W7bbuZNXc2dl+Xs1u1dwvvTCaFo3r06trx0znx8XFMfyJR+nTozM9u3Zg4Xffps7r0KYZvbp1pG/PLjzQp7unIqdjxf1+6dIlHu7Xm369utKnW0emfvBuhmW+/PxTHujVlQd6daVv907cW6sa586dBWD92kh6dm5H946t+WLaxx7LDdY7Xh7v24Qtc0azde4Ynri/CQATh3dh+7zn2DRrFLPeGExgkYBM2+794SU2zx7NhpnPsuarkanTq99amlVf/JfNs0cz9+0hFC3s74mH4hrJpZsnGWPy5C0hyRhXbxcSk02z5s3NvgNHzPmLl0yHDh3N7j370i2zbPkq8/DAQSb+corZuOVX0617j2y3ddfNqrmtnD2v5I5LTHH59suajWbTtl2mbdv2mc5/590PzIRJr5q4xBRz5PhJU7t2HXM6LtHEJaaYxk2amCMnTuVou1ffrLrfz8Qnu3w7fTHJ/H3ynDkTn2xizyWYrt26m9Ubtl5z+YVLfjJ9+/U3Z+KTzcm4S6Zps+Zm958HTcy5eNOuQ0ezddfeHOWw6vHiX/PxbN1qdR9vdu/72wTVG24K3z3MLN+wx1Tt9KJp/+i7pvDdw4x/zcfN69OWmdenLcu0/aG/T5rSTUZmmL5l9yHTYtBbxr/m4yZi7Jdm4tTF2c7kqfe8mLgkkxs3T75P56seiN27dhIeXo4y4eH4FShAm3btWbVyebplVq5YTsdOXRARatxZk7i488TGxmSrrebOP9mtmhugVu06BAYGXnsBES5evIgxhvj4eIoFBmKz5Y3RSqvudxGhUKHCACQnJ5OcnJzlSW/LliymVZt2APy+exdlwstSukw4fn4FaNm6LatXrfBEbMBax8ttt4SxadchEhKTsNtTiNz6F52b3snyDXux21MA2LTrIKVDi7u03srlQliz9S8AVmzYS5fmNXM5+b9nxQ6IfFVAxERHE1YqLPV+SGgo0dHR6ZeJiSY07J9lQkPDiImOzlZbd7FqbrBudqvmzo7efftx8OB+WjdvRO/unRjxzGh8fBxPdUF4fMgg+vXuxry5szyezcr73W6380CvrrRp1oC69epTrfqdmS6XmJDAhnWRNG3REsj4eEJCw4iNifFI5uzIS8fLb/uP06BWJUoEFibA3482DapSJiwo3TIPdr6XH9f+nml7YwyLPniCtV+NZGC3+1Kn/77/BB2aVAegW8talAkNyrS9N4nkzs2T8sbHklxiMBmmydV71GS+TLbauolVc4N1s1s1d3asX7uGKlVuZ8onX3Ds6BGGRgzkrlq1KVKkCNOmf01wSCinT51i6JCBlC9fgVq163gsm5X3u81mY8bs+cSdP8/Ip59k/1/7qFipcoblIlevokbNWgQGFndMyPTxuDmsC/LS8fLHwWje+Pwnvv/wCS4mXGLnn3+TnGxPnT9yUGvs9hRmLt6caftmD7/FidhzBAcV4fuPnuCPQ1Gs3bafIS9+xRsjezBqcFt++GUXl5Psmbb3Jj2J0stCQ8OIOhGVej8mOpqQkJB0y4SEhhEd9c8y0dFRBIeEZKutu1g1N1g3u1VzZ8fCBfNp1rwlIkJ42XLcXLoMhw4eACA4JBSAEiVL0rRZC3bv3unRbPlhvxctVoy7a9dh/drITOf/tPSf4QvI+HhioqO4KViPl2v54rv11L//FVoOepsz5y7y15FYAPp1vId2jaoxYMzn12x7IvYcALFnLrBwxU7qVC0PwJ+Houk49H3u6/cqs5du5eCxWHc/jBtCviogqlarzpEjhzh27ChJly+zdPEPNG7aLN0yTZo2Y9HC7zDGsHPHdooUKUpwcEi22mru/JPdqrmzIyysFJs2rgfg1KmTHD58kNJlwkmIj+fixQsAJMTHs2H9WipVutWj2ay638+cPk3c+fMAJCYmsmnjesrfUiHDchfi4vh162Yapcl1e9VqHD1ymON/HyMp6TI//biERo2beiR3duS14yU4qAgA4WFBdG52J7OXbqFl/dv574AW9Bg+hYTEpEzbFfIvQJFCBVP/bnHvbfy2/3i6dYoIzw5uzcdz17j9cbhKhzC8zNfXl1FjXuCxiEdISbHTpWt3KlWqzOxZ3wDQq3dfGjZqzJrVv9ChbUv8/QMYN35ilm01d/7MbtXcAKNHPs2WLZs5e/YMbVs0ZsjQYSQnJwPQo1cfBg95jLHPj6JXt45g4MnhIwgKCuLYsaOMGP4E4BjPb9O2A/UbNPRYbrDufj95MpZxz48iJSWFlJQUmrdqQ4NGTZg3ZyYA3Xr2AWDVip+pe+99BAQUSveYRzw7hicfG0xKSgodO3elgh4v1/TN649QonhhkpLtDJ88m7NxCbz1TC8KFvDl+w8deTbtOsSTE2ZSKjiQD164n67DPiSkZFFmvTkYAF+bjVlLtvDTuj0A9GpTmyG9GwGwYMV2pi/Y4PbHcSMQk8n4XF6QmJzJgKdS+Uyy3bqHua/NemO2AIl5cPw7u3x9rNlpHFxvmLcj5FjCr+955EA/E587LwZBhTz3xMxXPRBKKaWUFeWlE2uzSwsIpZRSysv0WxhKKaWUuiFoD4RSSinlZTqEoZRSSimXWbB+0AJCKaWU8joLVhB6DoRSSimlXKY9EEoppZSXWfFbGFpAKKWUUl5mxZModQhDKaWUUi7THgillFLKyyzYAaEFhFJKKeV1FqwgdAhDKaWU8jLJpf+ytS2RNiLyh4j8JSLP5jSzFhBKKaXUDUJEbMD7QFvgDqCviNyRk3VpAaGUUkp5mUju3LKhLvCXMeaAMeYyMBPonJPMefYciAA/GWKMmertHDkhIhFWzG7V3GDd7OLnY8ncYN19HuDna8ncYN19nrj9fUvm9iR/39w5C0JEIoCINJOmXrXvSwNH09w/BtyTk23l5R6IiOsvkmdZNbtVc4N1s1s1N1g3u1Vzg3WzWzW35Rhjphpjaqe5XV24ZVaomJxsKy8XEEoppZTKXceA8DT3ywDHc7IiLSCUUkqpG8dmoLKI3CIiBYA+wMKcrCjPngMBWHm8zKrZrZobrJvdqrnButmtmhusm92qufMdY0yyiDwB/AjYgGnGmN9ysi4xJkdDH0oppZS6gekQhlJKKaVcpgWEUkoppVymBYRSXiRixYv4WpOIFPZ2hpwSkTA9VlRek6cKCBGpIiL3ioif8+c2LcWimSuJSG0RKejtLK4Qkaoi0lhESno7i6tEpIGI9AcwxhgrvTGISEcR+Y+3c7hKRDoDr4hIiLezuEpEWgPzSf/VuzxPROqJSH/n/wt4O4/KfXmmgBCRbsACYDzwKfC4iBTzbqrsEZFbAYwxdisVESLSAZgHvAZ8fuVx5HUi0hb4BngKmC4iYV6OlC0i4iMiRYApwCgReRRSi4g881y8FhFpBbwM/O7tLK4QkcbAK8ACY0yMt/O4wrnPXwFKAf/1cpxsE5FOOL550QIYAZTzbiLlDnniRUtE/IDewCBjTHMchUQ4MDKvFxHON+HtIvI1WKeIEJH6wOvAQ8aYpsAZIMdXZfMUEWkCvAM8YozpAlwGqnkxUrYZY1KMMReAL3AUyfVF5Kkr87wa7jqcx8uXQIQx5icRCRSRciJSyNvZsuFu4BNn7ptFpKWI3CMigd4OlhURaQF8APQDKgO3i0gj76a6Pmev4OPA/caYh4DzQE0RCRERf++mU7kpTxQQTsVwPEnA0V33PVAAuD+vdvE6x1SfAIYDl0VkBliniAAmG2N+df49FihhgaGMaGCIMWaTs+fhHuAJEZkiIj3y6rFylWQcBfIXQF0ReVNEJolDXnpOpnUKSAJKOd8gvgM+xNFzldf3e3Kav+cCA3E8b98XkSDvRMoWG/Cg8zv6hYE/gKqQ58+dSQYCgNucHwCbAA8CbwPPWflcFJVennixMsYkAW8C3USkofPT2BpgO9DAm9myYoy5iOPF6Gsc3XT+aYsIb2bLho04hi+unLtREEc3YzHntDx5boExZo8xZqXz7iDgA2dPxAagJ3CTt7K5YAEQZYxZDmwBHgWKGYc82RNhjPkDaA+8BezAccx3AJYC3YG8/Ea8AhgsIjOBj40xfXEUzBdwXJkwTzLG/GiMWSciPsaYs8APwFgRqW7y8A/4GGPOAf8HjAKWAZ8ZYzoCn+D42eRKXoynclGeKCCcInEcbP1FpJExxm6M+Rq4GbjTu9GuzRhz3BhzwRhzEhgCBFwpIkSklojc5t2EmXPu3/POuwKcBU4bY2JFpB8wXkQCvBYwG4wxE4wx451/fwYUxRonmiUAVURkMI7iYTJQVkSGeDdW1owxO3AUDZOMMR87h2Sm4Sgeyno33bUZY3bjKPDvAW5xTjuA4xN+sBejZcuVotIYsxTHeQUd8nhvFcaYuTjOf4gEfnVOW4HjOarnQ+QTeeanrI0xiSLyFY6rgo1yvvFeAkKBE14Nl03GmFPON4HXRGQvjheopl6OdV3GmGTggogcFZFJQCtggDEmwcvRrklEJO2nMBHpjuNYydFFYTzJGHNcRI4CzwOPG2MWiUhT4C8vR7suY8zvpDmJ0rnfg8n7z9ElOHodXhSRw85pd+Eo3qxkB46Th1/N672cxpgzIrIC6CUilwF/HAXcTu8mU7klz/2UtfPrPvfh+DSfCLyTZpzeEpwnxj0DtDTG7PJ2nutxjqf6AXuc/29ujNnn3VTZ4zxn4wHgaaC389Nmnici4UCIMWar875PXh2+yIzzmHkYxyf7njn9LX1PE5FaQA8cQ3afW+H5eTURmQ2MNMYc8naW6xGR4jjOf+iO4/V8pLMnS+UDea6AuMI5Lp9nx4SvxXlS1mzgv8YYS1XaIjIA2GyVNwNI/QZPS2C/c5zeUq7uSbEKZwHRGMe5HHu9nedGYNVjBUBEiuJ4vzl/3YWVZeTZAsLKRMTfGJPo7RyusvILlFJKKc/SAkIppZRSLsuzZ/EqpZRSKu/SAkIppZRSLtMCQimllFIu0wJCKaWUUi7TAkKpXCAidhHZLiK7RWTOv7nIlIh8LiI9nH9/IiJ3ZLFsE+eFrlzdxiERyfCz39eafo11DBCR93Jju0op69ECQqnckWCMqWmMqYbjCqGPpp2Z04urGWMecf7647U0AVwuIJRS6t/SAkKp3BcJVHL2Dqx0Xup9l4jYROQ1EdksIjuvXPvCeV2D90TkdxH5AQi5siIRWSUitZ1/txGRbSKyQ0SWi0h5HIXKU87ej4YiEiwi3zq3sVlE7nO2LSkiy0TkVxGZguP6J9kiInVFZJ2z7ToRqZJmdriILBWRP0RkbJo2D4jIJmeuKTktoJRSeVeeuRaGUvmBiPgCbXFcpRIcV3usZow5KCIRwDljTB3nT3CvFZFlOK7JUAWojuN6Hr8D065abzDwMdDIua4SxpjTIvIRcMEY87pzua+Bt4wxa0SkLPAjcDuO60CsMcaME5H2QIQLD2uvc7vJItICmIjjp4lTHx8QD2x2FkAXgd7AfcaYJBH5AOgHTHdhm0qpPE4LCKVyR4CIbHf+HQl8imNoYZMx5qBzeiugxpXzG4BAoDLQCPjGeXGk484LEF2tHrD6yrqMMaevkaMFcIfjl6YBKOb8GeFGQDdn2x9E5IwLjy0Q+EJEKuO42J1fmnk/GWNOAYjIPKABkAzcjaOgAAgAYlzYnlLKArSAUCp3JBhjaqad4HzzvJh2EjDMGPPjVcu1w/HGnBXJxjLgGJa89+orqTqz5PRnZ18GVhpjujqHTValmXf1Oo0z6xfGmFE53J5SygL0HAilPOdH4DHnBcAQkVtFpDCwGujjPEeiFJlfAn490FhEbnG2LeGcHgcUTbPcMuCJK3dEpKbzz9U4hhEQkbZAkAu5A4G/nX8PuGpeSxEpISIBQBdgLbAc6CEiIVeyikg5F7anlLIALSCU8pxPcJzfsE1EdgNTcPQCzgf2AbuAD4Ffrm5ojInFcd7CPBHZAcxyzloEdL1yEiXwJFDbeZLm7/zzbZCXgEYisg3HUMqRLHLuFJFjztubwKvAJBFZC1x9MuQa4EtgO/CtMWaL81sjzwHLRGQn8BNQKnu7SCllFXoxLaWUUkq5THsglFJKKeUyLSCUUkop5TItIJRSSinlMi0glFJKKeUyLSCUUkop5TItIJRSSinlMi0glFJKKeWy/wef29JMxho51gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49  3  1  1  0  0  0  0  0]\n",
      " [ 5 46  2  0  0  1  0  0  0]\n",
      " [ 6  5 39  1  1  0  1  0  1]\n",
      " [ 2  5  4 38  0  5  0  0  0]\n",
      " [ 0  1  1  2 42  7  0  1  0]\n",
      " [ 2  3  1  2  2 33  3  1  1]\n",
      " [ 0  4  1  0  0  1 42  4  2]\n",
      " [ 0  0  0  0  0  0  4 50  0]\n",
      " [ 0  0  0  0  1  0  2  1 50]]\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader, device,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "tuys1RSKZTWK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "97+1+3+2+4+1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2N7fWF9Klf7w"
   },
   "source": [
    "학습된 모델의 예측 결과를 시각화하면 다음과 같습니다. 괄호안에 'O'가 있는 경우, 모델이 정확한 예측을 한 것이고 'X'인 경우는 틀린 예측을 한 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 3814,
     "status": "ok",
     "timestamp": 1673501765056,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "0G0WOvWllf7w",
    "outputId": "ea91823d-a695-4269-9793-26540883a72d"
   },
   "outputs": [],
   "source": [
    "columns = 5\n",
    "rows = 5\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "model.eval()\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "for i in range(1, columns*rows+1):\n",
    "    data_idx = np.random.randint(len(test_data))\n",
    "    input_img = test_data[data_idx][0].unsqueeze(dim=0).to(device) \n",
    "\n",
    "\n",
    "    output = model(input_img)\n",
    "    _, argmax = torch.max(output, 1)\n",
    "    pred = argmax.item()\n",
    "    label = test_data[data_idx][1]\n",
    "    \n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    \n",
    "    if pred == 0 :\n",
    "      pred_title=\"A3\"\n",
    "    elif pred == 1 :\n",
    "      pred_title=\"A4\"\n",
    "    else:\n",
    "      pred_title=\"A5\"\n",
    "\n",
    "\n",
    "    if pred == label:\n",
    "        plt.title(pred_title + '(O)')\n",
    "    else:\n",
    "        plt.title(pred_title + '(X)')\n",
    "    plot_img = test_data[data_idx][0]\n",
    "    # 이미지를 normalization 이전 상태로 되돌리는 작업\n",
    "    plot_img[0, :, :] = plot_img[0, :, :] * std[0] + mean[0]\n",
    "    plot_img[1, :, :] = plot_img[1, :, :] * std[1] + mean[1]\n",
    "    plot_img[2, :, :] = plot_img[2, :, :] * std[2] + mean[2]\n",
    "    plot_img = transforms.functional.to_pil_image(plot_img)\n",
    "    plt.imshow(plot_img)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKf9kBTllf7w"
   },
   "source": [
    "## 11. Transfer Learning\n",
    "\n",
    "실습을 끝내기전에, 우리는 **전이 학습(transfer learning)** 을 구현해 보고 성능을 확인해 볼 것입니다. \n",
    "전이학습이란 비슷한 목적으로 미리 학습된 모델의 파라미터로 나의 모델의 파라미터를 초기화한 후 학습을 이어서 진행하는 것을 말합니다. 그렇다면 이러한 전이학습은 어떤 장점이 있기에 사용하는 것일까요?\n",
    "\n",
    "여러분이 현실에서 딥러닝을 활용할 때 흔히 마주칠 수 있는 현실적인 제약들이 있습니다. 데이터 부족, 컴퓨팅 리소스 부족, 시간의 부족이 대표적인 현실적인 제약들에 속합니다. 우리가 풀고자 하는 문제와 완전히 똑같지는 않지만 어느정도 연관성이 있는 문제를, 아주 많은 양의 데이터로, 미리 학습한 모델이 있다면 그 모델은 정말 아무것도 모르는 백지 상태의 모델보다 우리가 풀고 싶은 문제에 대해 훨씬 더 빨리 배우고 잘 배울 수 있습니다. \n",
    "\n",
    "구현은 전혀 어렵지 않습니다. 먼저 미리 학습된 모델을 불러와야 합니다. \n",
    "여기서 불러올 모델은 ResNet으로, 대규모 ImageNet 데이터로 이미지 분류를 학습한 모델입니다.\n",
    "\n",
    "ResNet에 대해 잘 기억이 나지 않는다면 ['Lab-10-6'](https://www.youtube.com/watch?v=Qb_bYWcQXqY&list=PLQ28Nx3M4JrhkqBVIXg-i5_CVVoS1UzAv&index=25) 강의를 참고하시기 바랍니다.\n",
    "\n",
    "다음 코드블록은 torchvision에 구현된 resnet50을 파라미터가 학습된 상태로 불러옵니다. 학습된 파라미터를 다운받기 위해 몇 분의 시간이 소요될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1463,
     "status": "ok",
     "timestamp": 1673502804913,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "5GOhVIiWlf7x",
    "outputId": "e4f7d602-261d-4794-a8cd-1d55b237d60b"
   },
   "outputs": [],
   "source": [
    "new_model = torchvision.models.resnet50(pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nltog0G-lf7x"
   },
   "source": [
    "다만 우리가 앞서 직접 정의한 모델(SimpleCNN)에서는 입력 이미지의 크기를 120x120로 한 것에 반해, 방금 불러온 ResNet은 입력 이미지 크기를 최소한 224x224로 가정하고 학습된 모델입니다. 따라서 입력 데이터 크기만 수정하여 DataLoader를 다시 정의하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOr025BTlf7x"
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize([224, 224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "train_data = CatDogDataset(data_dir='./data/action_level', mode='train', transform=data_transforms['train'])\n",
    "val_data = CatDogDataset(data_dir='./data/action_level', mode='val', transform=data_transforms['val'])\n",
    "test_data = CatDogDataset(data_dir='./data/action_level', mode='test', transform=data_transforms['val'])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtqvu5vGlf7y"
   },
   "source": [
    "우리가 불러온 ImageNet에 학습된 ResNet은 1000개의 class를 구분하는 네트워크입니다. 즉, 마지막 FC layer의 출력 뉴런 수가 1000개인 것입니다. 우리는 2개의 class를 구분하는 네트워크를 원하기 때문에, 마지막 FC layer만 출력을 2로 바꿔주고 이 부분에 대해서만 학습을 추가로 진행하면 됩니다.\n",
    "\n",
    "상황에 따라서는 우리가 이번에 하는 것처럼 마지막 FC layer만 학습을 진행하는 것이 아니라 전체 네트워크에 대해서 학습을 이어서 진행하는 경우도 있습니다. 이를 파라미터 fine tuning(미세 조정)이라고 합니다. ImageNet에는 다양한 동물 class도 포함이 되어있습니다. 즉, 우리가 불러온 ResNet은 강아지와 고양이 같은 동물에 대한 특징을 이미 어느정도 잘 추출하는 네트워크인 것입니다. 따라서 fine tuning이 굳이 필요하지 않습니다. 게다가, 이번 실습과 같이 적은 양의 데이터를 통해 모델을 학습시키는 상황에서 fine tuning을 진행하는 것은 overfitting의 가능성이 커지는 것이기 때문에 오히려 성능을 낮추는 결과를 가져올 수 있습니다. \n",
    "\n",
    "이러한 내용을 아래 코드에 구현해두었습니다.\n",
    "- [`nn.Module.parameters`](https://pytorch.org/docs/stable/nn.html?highlight=parameters#torch.nn.Module.parameters)를 통해 모델 파라미터에 대한 iterator를 가져올 수 있습니다. 먼저, 이 iterator를 통해 for문 을 돌며 모든 파라미터에 대해 [requires_grad](https://pytorch.org/docs/stable/autograd.html?highlight=requires_grad#torch.Tensor.requires_grad)를 False로 바꿔줍니다. 이렇게 하면 이 파라미터들에 대해서는 기울기가 계산되지 않기 때문에, 파라미터 업데이트가 되지 않습니다.\n",
    "- 그 다음으로는 맨마지막 FC layer를 새로 정의해주는 것입니다. 그런데 문제는 마지막 FC layer에 어떻게 접근하느냐 입니다. 우리가 구현을 하지 않았기 때문에 마지막 layer의 입력 뉴런수가 어떻게 되는지도 모르는 상황입니다. 그러면 우리가 할 일은 구현된 코드를 보는 것입니다. torchvision에 우리가 불러온 ResNet이 구현되어 있습니다. [이곳](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L146)에 가면 구현된 ResNet의 마지막 레이어의 변수명이 **self.fc**로 되어있음을 알 수 있습니다. 따라서 우리는 **모델.fc**와 같은 방식으로 이 layer에 접근할 수 있습니다. 이러한 방식으로 마지막 FC layer의 입력 뉴런수를 가져오고, 출력 뉴런 수는 우리의 문제에 맞게 2로 하여 마지막 layer를 수정할 수 있습니다. \n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "다음을 읽고 코드를 완성해보세요.\n",
    "\n",
    "1. 불러온 모델의 가장 마지막 FC layer의 이름은 'fc'임을 확인했습니다. 해당 레이어의 입력 뉴런 수를 `num_ftrs` 변수에 저장했습니다. 이제, 이 마지막 레이어에 새로운 FC layer를 정의하고, 그것의 입력 뉴런 수는 이전과 마찬가지로 `num_ftrs`로 하고 출력 뉴런 수는 우리가 분류하고자 하는 class의 총 개수로 해주세요,\n",
    "2. 손실함수를 Cross Entropy Loss Function을 선언하고 이를 `criterion` 변수에 담습니다.\n",
    "3. Adam 옵티마이저를 선언하고 이를 `optimizer` 변수에 담습니다. 인자로 넣어 주는 모델 파라미터는 새로 불러온 모델의 파라미터여야 함에 주의하세요. **<3. 하이퍼파라미터 세팅>** 에서 정의한 `learning_rate` 를 사용하세요.\n",
    "\n",
    "**ResNet 의 최종 출력층을 바꾸고 손실함수 및 옵티마이저를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1673448797075,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "LFRYn9yXUVw8",
    "outputId": "7a2dee1f-e918-452f-9c38-20068deab46a"
   },
   "outputs": [],
   "source": [
    "new_model.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3U1GCEt9lf7y"
   },
   "outputs": [],
   "source": [
    "for param in new_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = new_model.fc.in_features\n",
    "## 코드 시작 ##\n",
    "new_model.fc = nn.Linear(num_ftrs,3)    # 위의 설명 1. 을 참고하여 None을 채우세요.\n",
    "criterion = nn.CrossEntropyLoss()      # 위의 설명 2. 를 참고하여 None을 채우세요.\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)       # 위의 설명 2. 를 참고하여 None을 채우세요.\n",
    "new_model = new_model.to(device)\n",
    "     # 위의 설명 3. 을 참고하여 None을 채우세요.\n",
    "## 코드 종료 ##\n",
    "val_every = 1\n",
    "saved_dir = './saved/ResNet50'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSiNs6n5lf7z"
   },
   "source": [
    "아래의 코드를 실행해 코드를 성공적으로 완성했는지 확인해보세요. \n",
    "\n",
    "별다른 문제가 없다면 이어서 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1673429420489,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "09Ovcjhklf7z",
    "outputId": "fa283966-b4f1-4c4e-ba1b-2effae0e7995"
   },
   "outputs": [],
   "source": [
    "checker.final_fc_check(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sd3zkOLNlf7z"
   },
   "source": [
    "불러온 모델에 대해 학습을 진행합니다. 이 과정도 100분 내외의 시간이 소요됩니다. 마찬가지로 적당히 학습이 진행되는 것만 보고 중단후 다음 단계로 넘어가셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOETBvHoTR9A"
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "7xY9VVbslf70",
    "outputId": "f11ce7b0-0bf5-4b98-9edd-295550174f2a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(num_epochs, new_model, train_loader, criterion, optimizer, saved_dir, val_every, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvtxYdl_lf70"
   },
   "source": [
    "전이 학습을 마친 후, 훈련 단계에서 최고 성능을 보인 파라미터를 불러오고, 그 성능을 확인합니다. 마찬가지로 위에서 학습을 끝까지 진행하지 않았다면 아래 주석부분을 해제하시기 바랍니다.\n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "다음을 읽고 코드를 완성해보세요. \n",
    "\n",
    "1. `model_path`의 경로에 있는 모델 파일을 로드하여, 이를 `check_point` 변수에 저장합니다. 또한, 미리저장된 모델이 GPU로 학습했는데 CPU 로 불러올 경우, 파이토치에서 모델을 불러오는 함수에 `map_location` 인자에 `device` 정보를 전달해야 적용됩니다. (즉, 함수에 `map_location=device` 가 되어야 합니다.) `device` 변수는 **<1. Package load>** 에서 이미 선언했습니다.\n",
    "2. `check_point` 딕셔너리에 접근하여 모델의 파라미터를 `state_dict` 변수에 저장합니다. 접근을 위한 딕셔너리의 키값은 'net' 입니다.\n",
    "3. `state_dict`의 파라미터들을 ResNet 모델(`new_model`)에 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1673429668973,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "zNv-lqc9lf71",
    "outputId": "91a7687a-3204-4826-9b3f-e38e2e9f58a6"
   },
   "outputs": [],
   "source": [
    "# model_path = './saved/ResNet50/best_model.pt'\n",
    "model_path = './saved/pretrained/ResNet50/best_model.pt' # 모델 학습을 끝까지 진행하지 않은 경우에 사용\n",
    "\n",
    "## 코드 시작 ##\n",
    "checkpoint = torch.load(model_path,map_location=device)   # 위의 설명 1. 을 참고하여 None을 채우세요.\n",
    "state_dict = checkpoint['net']   # 위의 설명 2. 를 참고하여 None을 채우세요.\n",
    "new_model.load_state_dict(state_dict)                  # 위의 설명 3. 을 참고하여 None을 채우세요.\n",
    "## 코드 종료 ##                # 위의 설명 3. 을 참고하여 None을 채우세요.\n",
    "## 코드 종료 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhdjwUjYlf71"
   },
   "source": [
    "테스트를 수행합니다. 정확도가 97% 내외가 나온다면 성공적으로 진행된 것입니다. 겨우 마지막 FC layer만 학습시켰음에도 불구하고 우리의 SimpleCNN보다 성능이 훨씬 좋은 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17382,
     "status": "ok",
     "timestamp": 1673429690350,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "FISuUOnAlf71",
    "outputId": "9301cfd4-3fbf-47c4-bf06-c1618713ce8a"
   },
   "outputs": [],
   "source": [
    "test(new_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mL0QTE98lf72"
   },
   "source": [
    "학습된 모델의 예측 결과를 시각화하면 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 3906,
     "status": "ok",
     "timestamp": 1673429698017,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "Zm6yUqO3lf73",
    "outputId": "6914b5f3-592f-49a9-cb2f-38866bcaae5f"
   },
   "outputs": [],
   "source": [
    "columns = 5\n",
    "rows = 5\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "model.eval()\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "for i in range(1, columns*rows+1):\n",
    "    data_idx = np.random.randint(len(test_data))\n",
    "    input_img = test_data[data_idx][0].unsqueeze(dim=0).to(device) \n",
    "    '''\n",
    "    unsqueeze()를 통해 입력 이미지의 shape을 (1, 28, 28)에서 (1, 1, 28, 28)로 변환. \n",
    "    모델에 들어가는 입력 이미지의 shape은 (batch_size, channel, width, height) 되어야 함에 주의하세요!\n",
    "    '''\n",
    "    output = new_model(input_img)\n",
    "    _, argmax = torch.max(output, 1)\n",
    "    pred = argmax.item()\n",
    "    label = test_data[data_idx][1]\n",
    "    \n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    pred_title = 'Cat' if pred == 0 else 'Dog'\n",
    "    if pred == label:\n",
    "        plt.title(pred_title + '(O)')\n",
    "    else:\n",
    "        plt.title(pred_title + '(X)')\n",
    "    plot_img = test_data[data_idx][0]\n",
    "    # 이미지를 normalization 이전 상태로 되돌리는 작업\n",
    "    plot_img[0, :, :] = plot_img[0, :, :] * std[0] + mean[0]\n",
    "    plot_img[1, :, :] = plot_img[1, :, :] * std[1] + mean[1]\n",
    "    plot_img[2, :, :] = plot_img[2, :, :] * std[2] + mean[2]\n",
    "    plot_img = transforms.functional.to_pil_image(plot_img)\n",
    "    plt.imshow(plot_img)\n",
    "    plt.axis('off')\n",
    "model.train()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut7oRMWslf74"
   },
   "source": [
    "## 12. Summary\n",
    "\n",
    "여기까지 오신 여러분 잘하셨습니다! \n",
    "\n",
    "우리는 이번 실습을 통해 다음과 같은 내용을 학습했습니다.\n",
    "\n",
    "- Dataset class를 우리가 가진 데이터셋에 맞게 customize 하여 정의할 수 있다.\n",
    "- CNN을 설계하고 이미지 분류기를 학습시킬 수 있다.\n",
    "- 학습된 모델을 저장하고 불러올 수 있다.\n",
    "- 데이터, 리소스, 시간이 부족한 상황에서 전이학습을 사용하여 이를 극복할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot3C1_tjlf74"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmEZhMYolf74"
   },
   "source": [
    "# Self-Review\n",
    "\n",
    "학습 환경에 맞춰 알맞는 제출방법을 실행하세요!\n",
    "\n",
    "### 로컬 환경 실행자\n",
    "\n",
    "1. 모든 실습 완료 후, Jupyter Notebook 을 `Ctrl+S` 혹은 `File > Save and checkpoint`로 저장합니다.\n",
    "2. 제일 하단의 코드를 실행합니다. 주의할 점은 Jupyter Notebook 의 파일이름을 수정하시면 안됩니다! 만약에 노트북 이름을 수정했다면 \"pytorch-cnn-project\" 로 바꿔주시길 바랍니다. 모든 평가 기준을 통과하면, 함수 실행 후 프로젝트 \"submit\" 디렉토리와 압축된 \"submit.zip\"이 생깁니다. \"cnn_submission.tsv\" 파일을 열고 모두 Pass 했는지 확인해보세요!\n",
    "    * \"cnn_submission.tsv\" : 평가 기준표에 근거해 각 세부항목의 통과여부(Pass/Fail) 파일\n",
    "    * \"cnn_submission.html\" : 여러분이 작성한 Jupyter Notebook 을 html 형식으로 전환한 파일\n",
    "3. 코드 실행결과 안내에 따라서 `submit.zip` 파일을 확인하시고 제출해주시길 바랍니다.\n",
    "\n",
    "### Colab 환경 실행자\n",
    "\n",
    "1. 모든 실습 완료 후, Jupyter Notebook 을 `Ctrl+S` 로 저장합니다.\n",
    "2. 제일 하단의 코드를 실행합니다. 코드 실행결과 안내에 따라서 재작성하거나 다음스텝으로 넘어갑니다. 모든 평가 기준을 통과하면, 함수 실행 후 프로젝트 \"submit\" 디렉토리와 압축된 \"cnn_submission.tsv\"만 생깁니다. \"cnn_submission.tsv\" 파일을 열고 모두 Pass 했는지 확인해보세요!\n",
    "    * \"cnn_submission.tsv\" : 평가 기준표에 근거해 각 세부항목의 통과여부(Pass/Fail) 파일\n",
    "3. 프로젝트를 저장한 드라이브의 `submit` 폴더에서 `cnn_submission.tsv` 파일을 다운 받습니다.\n",
    "4. Colab Notebook 에서 `파일 > .ipynb 다운로드`를 통해서 노트북을 다운로드 받습니다.\n",
    "5. 로컬에서 Jupyter Notebook 프로그램을 실행시킵니다. \n",
    "6. 4번 스텝에서 다운받은 노트북을 열고 `File > Download as > HTML(.html)` 로 재 다운로드 합니다.\n",
    "7. 3번 스텝에서 받은 파일과 6번 스텝에서 받은 파일을 하나의 폴더에 넣고, `submit.zip` 이라는 이름으로 압축하고 제출해주시길 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2052,
     "status": "ok",
     "timestamp": 1673429707126,
     "user": {
      "displayName": "박재석",
      "userId": "08043462631904569995"
     },
     "user_tz": -540
    },
    "id": "gJK8MnNulf75",
    "outputId": "d13b6f0f-0566-41cf-b76e-0c3a8d56a9b4"
   },
   "outputs": [],
   "source": [
    "import check_util.submit as submit\n",
    "submit.process_submit()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "목차",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
